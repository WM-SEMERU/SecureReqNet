{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import sys\n",
    "import os.path\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\"@danaderp May'20 Refactoring for enhancing time complexity with pandas vectorization\"\n",
    "\n",
    "class Dynamic_Dataset:\n",
    "\t\"\"\"\n",
    "\tThis class efficiently 'stores' a dataset. Only a list of filenames and\n",
    "\tmappings to their ground truth values are stored in memory. The file\n",
    "\tcontents are only brought into memory when requested.\n",
    "\n",
    "\tThis class supports indexing, slicing, and iteration.\n",
    "\n",
    "\tA user can treat an instance of this class exactly as they would a list.\n",
    "\tIndexing an instance of this class will return a tuple consisting of\n",
    "\tthe ground truth value and the file content of the filename at that index.\n",
    "\n",
    "\tA user can request the filename at an index with get_id(index)\n",
    "\n",
    "\tExample:\n",
    "\n",
    "\t\tdataset = Dynamic_Dataset(ground_truth)\n",
    "\n",
    "\t\tprint(dataset.get_id(0))\n",
    "\t\t\t-> gitlab_79.txt\n",
    "\n",
    "\t\tprint(dataset[0])\n",
    "\t\t\t-> ('(1,0)', 'The currently used Rails version, in the stable ...\n",
    "\n",
    "\t\tfor x in dataset[2:4]:\n",
    "\t\t\tprint(x)\n",
    "\t\t\t\t-> ('(1,0)', \"'In my attempt to add 2 factor authentication ...\n",
    "\t\t\t\t-> ('(1,0)', 'We just had an admin accidentally push to a ...\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, ground_truth, path, isZip):\n",
    "\t\t'''\n",
    "\t\t@param ground_truth (dict): A dictionary mapping filenames to ground truth values\n",
    "\t\t'''\n",
    "\t\tself.__keys = list(ground_truth.keys())\n",
    "\t\tself.__ground_truth = ground_truth\n",
    "\t\tself.__path = path\n",
    "\t\tself.__isZip = isZip\n",
    "\n",
    "\tdef __get_issue(self, filename):\n",
    "\t\tif self.__isZip:\n",
    "\t\t\tpaths = [str(x) for x in Path(self.__path).glob(\"**/*.zip\")]\n",
    "\t\t\tfor onezipath in paths:\n",
    "\t\t\t\tarchive = zipfile.ZipFile( onezipath, 'r')\n",
    "\t\t\t\tcontents = archive.read('issues/' + filename)\n",
    "\t\telse:\n",
    "\t\t\twith open(self.__path+'issues/' + filename, 'r') as file:\n",
    "\t\t\t\tcontents = file.read()\n",
    "\t\treturn contents.strip()\n",
    "\n",
    "\tdef get_id(self, index):\n",
    "\t\treturn self.__keys[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.__keys)\n",
    "\n",
    "\tdef __setitem__(self, key, item):\n",
    "\t\traise ValueError\n",
    "\n",
    "\tdef __getitem__(self, key):\n",
    "\t\tif type(key) == slice:\n",
    "\t\t\tnew_keys = self.__keys[key.start:key.stop:key.step]\n",
    "\t\t\tnew_gt = dict()\n",
    "\t\t\tfor key in new_keys:\n",
    "\t\t\t\tnew_gt[key] = self.__ground_truth[key]\n",
    "\t\t\treturn Dynamic_Dataset(new_gt)\n",
    "\t\telse:\n",
    "\t\t\tid = self.__keys[key]\n",
    "\t\t\treturn (self.__ground_truth[id], self.__get_issue(id))\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\tself.__index = 0\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __next__(self):\n",
    "\t\tif self.__index < len(self.__keys):\n",
    "\t\t\tto_return = self[self.__index]\n",
    "\t\t\tself.__index += 1\n",
    "\t\t\treturn to_return\n",
    "\t\telse:\n",
    "\t\t\traise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Dynamic_Dataset\" class=\"doc_header\"><code>class</code> <code>Dynamic_Dataset</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Dynamic_Dataset</code>(**`ground_truth`**, **`path`**, **`isZip`**)\n",
       "\n",
       "This class efficiently 'stores' a dataset. Only a list of filenames and\n",
       "mappings to their ground truth values are stored in memory. The file\n",
       "contents are only brought into memory when requested.\n",
       "\n",
       "This class supports indexing, slicing, and iteration.\n",
       "\n",
       "A user can treat an instance of this class exactly as they would a list.\n",
       "Indexing an instance of this class will return a tuple consisting of\n",
       "the ground truth value and the file content of the filename at that index.\n",
       "\n",
       "A user can request the filename at an index with get_id(index)\n",
       "\n",
       "Example:\n",
       "\n",
       "        dataset = Dynamic_Dataset(ground_truth)\n",
       "\n",
       "        print(dataset.get_id(0))\n",
       "                -> gitlab_79.txt\n",
       "\n",
       "        print(dataset[0])\n",
       "                -> ('(1,0)', 'The currently used Rails version, in the stable ...\n",
       "\n",
       "        for x in dataset[2:4]:\n",
       "                print(x)\n",
       "                        -> ('(1,0)', \"'In my attempt to add 2 factor authentication ...\n",
       "                        -> ('(1,0)', 'We just had an admin accidentally push to a ..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Dynamic_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Processing_Dataset:\n",
    "    \"\"\"\n",
    "    A class to wrap up processing functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.__path = path\n",
    "\n",
    "    def get_issue(self, filename):\n",
    "        with open('combined_dataset/issues/' + filename, 'r') as file:\n",
    "            contents = file.read()\n",
    "        return contents.strip()\n",
    "\n",
    "    def get_ground_truth(self):\n",
    "        gt = dict()\n",
    "        #print(sys.path[0])\n",
    "        #path = \"combined_dataset/full_ground_truth.txt\"\n",
    "        #path = os.path.join(sys.path[0], path)\n",
    "        with open(self.__path+'full_ground_truth.txt') as gt_file:\n",
    "            for line in gt_file.readlines():\n",
    "                tokens = line.split()\n",
    "                filename = tokens[0]\n",
    "                security_status = tokens[1]\n",
    "                if filename in gt:\n",
    "                    raise KeyError(\"Invalid Ground Truth: Duplicate issue [{}]\".format(filename))\n",
    "                gt[filename] = security_status\n",
    "        return gt\n",
    "\n",
    "    def get_test_and_training(self, ground_truth, test_ratio=0.1, isZip = False):\n",
    "        ids = list(ground_truth.keys())\n",
    "        sr = []\n",
    "        nsr = []\n",
    "\n",
    "        for id in ids:\n",
    "            if ground_truth[id] == '(1,0)':\n",
    "                sr.append(id)\n",
    "            elif ground_truth[id] == '(0,1)':\n",
    "                nsr.append(id)\n",
    "            else:\n",
    "                raise ValueError(\"There was an issue with ground truth: {} - {}\".format(id, ground_truth[id]))\n",
    "\n",
    "\n",
    "        n_test = int(len(sr) * test_ratio)\n",
    "        sr_test = random.sample(sr, n_test)\n",
    "        nsr_test = random.sample(nsr, n_test)\n",
    "\n",
    "        test_gt = dict()\n",
    "        train_gt = dict(ground_truth)\n",
    "\n",
    "        for i in range(n_test):\n",
    "            sr.remove(sr_test[i])\n",
    "            test_gt[sr_test[i]] = '(1,0)'\n",
    "            del train_gt[sr_test[i]]\n",
    "\n",
    "            nsr.remove(nsr_test[i])\n",
    "            test_gt[nsr_test[i]] = '(0,1)'\n",
    "            del train_gt[nsr_test[i]]\n",
    "\n",
    "        test = Dynamic_Dataset(test_gt,self.__path, isZip)\n",
    "        train = Dynamic_Dataset(train_gt,self.__path, isZip)\n",
    "\n",
    "        return (test, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Processing_Dataset\" class=\"doc_header\"><code>class</code> <code>Processing_Dataset</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Processing_Dataset</code>(**`path`**)\n",
       "\n",
       "A class to wrap up processing functions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Processing_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing. RC leaving for testing crew to use as a reference\n",
    "#if __name__ == '__main__':\n",
    "#ground_truth = get_ground_truth()\n",
    "#dataset = Dynamic_Dataset(ground_truth)\n",
    "\n",
    "#test, train = get_test_and_training(ground_truth)\n",
    "\n",
    "#print(test[0])\n",
    "#print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy\n",
    "import pandas\n",
    "import re\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "class Embeddings:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__wpt = nltk.WordPunctTokenizer()\n",
    "        self.__stop_words = nltk.corpus.stopwords.words('english')\n",
    "        self.__remove_terms = punctuation + '0123456789'\n",
    "\n",
    "    def __split_camel_case_token(self, token):\n",
    "        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token).split()\n",
    "\n",
    "    def __clean_punctuation(self, token):\n",
    "        remove_terms = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'\n",
    "        cleaned = token\n",
    "        for p in remove_terms:\n",
    "            cleaned = cleaned.replace(p, ' ')\n",
    "        return cleaned.split()\n",
    "\n",
    "    def __clean(self, token):\n",
    "        to_return = self.__clean_punctuation(token)\n",
    "        new_tokens = []\n",
    "        for t in to_return:\n",
    "            new_tokens += self.__split_camel_case_token(t)\n",
    "        to_return = new_tokens\n",
    "        return to_return\n",
    "\n",
    "\n",
    "    def __normalize_document(self, doc):\n",
    "        # lower case and remove special characters\\whitespaces\n",
    "        doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        # tokenize document\n",
    "        tokens = self.__wpt.tokenize(doc)\n",
    "        #Filter stopwords out of document\n",
    "        filtered_tokens = [token for token in tokens if token not in self.__stop_words]\n",
    "        #Filtering Stemmings\n",
    "        filtered_tokens = [englishStemmer.stem(token) for token in filtered_tokens]\n",
    "        #Filtering remove-terms\n",
    "        filtered_tokens = [token for token in filtered_tokens if token not in self.__remove_terms and len(token)>2]\n",
    "        # re-create document from filtered tokens\n",
    "        return filtered_tokens\n",
    "\n",
    "    def preprocess(self, sentence, vocab_set=None):\n",
    "        tokens = sentence.split()\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            new_tokens += self.__clean(token)\n",
    "        tokens = new_tokens\n",
    "\n",
    "        tokens = self.__normalize_document(' '.join(tokens))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def get_embeddings_dict(self, embeddings_filename):\n",
    "        embeddings_df = pandas.read_csv(embeddings_filename)\n",
    "        embeddings_dict = dict()\n",
    "        for col in list(embeddings_df)[1:]:\n",
    "            embeddings_dict[col] = list(embeddings_df[col])\n",
    "        return embeddings_dict\n",
    "\n",
    "    def vectorize(self, sentence, embeddings_dict):\n",
    "        processed_sentence = self.preprocess(sentence)\n",
    "\n",
    "        matrix = []\n",
    "        for token in processed_sentence:\n",
    "            if token in embeddings_dict:\n",
    "                matrix.insert(0, embeddings_dict[token])\n",
    "        return numpy.matrix(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Embeddings\" class=\"doc_header\"><code>class</code> <code>Embeddings</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Embeddings</code>()\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing, left here for reference purposes\n",
    "#sentence = \"AAA AAA xxx BBB yyy CCC\"\n",
    "#embeddings = Embeddings()\n",
    "#embeddings_dict = embeddings.get_embeddings_dict('test.csv')\n",
    "#print(embeddings_dict)\n",
    "#print(vectorize(sentence, embeddings_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
