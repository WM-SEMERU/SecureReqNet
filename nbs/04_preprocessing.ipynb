{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "#danaderp May6'19\n",
    "#Prediction For Main Issues Data Set\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dot, Input, Dense, Reshape, LSTM, Conv2D, Flatten, MaxPooling1D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.layers import Embedding, Multiply, Subtract\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "__file__ = (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "# visualize model structure\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from securereqnet.utils import Dynamic_Dataset, Processing_Dataset\n",
    "from securereqnet.utils import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def __read_dataset(path):\n",
    "    process_unit = Processing_Dataset(path)\n",
    "    ground_truth = process_unit.get_ground_truth()\n",
    "    dataset = Dynamic_Dataset(ground_truth, path,False) # I'm not sure this needs to be False. RC\n",
    "    return process_unit, ground_truth, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def __train_test_split(process_unit, ground_truth,isZipFile):\n",
    "    test, train = process_unit.get_test_and_training(ground_truth,isZip = isZipFile)\n",
    "    return test,train\n",
    "#As the data is stored in a zip file isZip = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Train/Test split verification\n",
    "#for elem in test:\n",
    "#    print(elem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/roger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exporti\n",
    "#Added due to a lookup error in the next cell\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "__file__ = (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def __create_corpora(test,train):\n",
    "    '''Creates the corpora for training. Returns corpora_train,corpora_test,target_train, test_train,\n",
    "    max_len_sentences,embed_size'''\n",
    "    embeddings = Embeddings()\n",
    "    max_words = 5000 #<------- [Parameter]\n",
    "    pre_corpora_train = [doc for doc in train if len(doc[1])< max_words]\n",
    "    pre_corpora_test = [doc for doc in test if len(doc[1])< max_words]\n",
    "\n",
    "    embed_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),'data',\"word_embeddings-embed_size_100-epochs_100.csv\")\n",
    "    embeddings_dict = embeddings.get_embeddings_dict(embed_path)\n",
    "\n",
    "    # .decode(\"utf-8\") takes the doc's which are saved as byte files and converts them into strings for tokenization\n",
    "    corpora_train = [embeddings.vectorize(doc[1].decode(\"utf-8\"), embeddings_dict) for doc in pre_corpora_train]#vectorization Inputs\n",
    "    corpora_test = [embeddings.vectorize(doc[1].decode(\"utf-8\"), embeddings_dict) for doc in pre_corpora_test]#vectorization\n",
    "\n",
    "    target_train = [[int(list(doc[0])[1]),int(list(doc[0])[3])] for doc in pre_corpora_train]#vectorization Output\n",
    "    target_test = [[int(list(doc[0])[1]),int(list(doc[0])[3])]for doc in pre_corpora_test]#vectorization Output\n",
    "    #target_train\n",
    "\n",
    "    max_len_sentences_train = max([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "    max_len_sentences_test = max([len(doc) for doc in corpora_test]) #<------- [Parameter]\n",
    "\n",
    "    max_len_sentences = max(max_len_sentences_train,max_len_sentences_test)\n",
    "    print(\"Max. Sentence # words:\",max_len_sentences)\n",
    "\n",
    "    min_len_sentences_train = min([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "    min_len_sentences_test = min([len(doc) for doc in corpora_test]) #<------- [Parameter]\n",
    "\n",
    "    min_len_sentences = max(min_len_sentences_train,min_len_sentences_test)\n",
    "    print(\"Mix. Sentence # words:\",min_len_sentences)\n",
    "\n",
    "    embed_size = np.size(corpora_train[0][0])\n",
    "    \n",
    "    return corpora_train,corpora_test,target_train,target_test,max_len_sentences,embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def __get_training_corpora(path,isZip):\n",
    "    process_unit, ground_truth, dataset = __read_dataset(path)\n",
    "    test, train = __train_test_split(process_unit, ground_truth,isZip)\n",
    "    return __create_corpora(test,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Data set organization\n",
    "from tempfile import mkdtemp\n",
    "import os.path as path\n",
    "def process_corpora(data_path,isZip=True,save_file=False,save_path=\"\",name=\"\"):\n",
    "    '''\n",
    "    Process the corpora data for model training. Takes in corpora_train,corpora_test,max_len_sentences,save,path,name.\n",
    "    \n",
    "    @param save_file (bool): Determine if the data should be saved on disk.\n",
    "    \n",
    "    @param data_path (string): Path to the dataset to process\n",
    "    \n",
    "    @param save_path (string): Path to where the processed dataset should be save to.\n",
    "    \n",
    "    @param isZip: True if data is in a zipped file\n",
    "    \n",
    "    @param name (string): Name of the model used to name files saved.   \n",
    "    \n",
    "    returns train_x, test_x, train_y, test_y\n",
    "    '''\n",
    "    \n",
    "    corpora_train,corpora_test,target_train,target_test,max_len_sentences,embeddigs_cols = __get_training_corpora(data_path,isZip)\n",
    "    \n",
    "    #As the current shape of securereqnet is built of these values overwrite whatever the processed data would want\n",
    "    max_len_sentences = 618\n",
    "    embeddigs_cols = 100\n",
    "    \n",
    "    #Memoization\n",
    "    file_corpora_train_x = path.join(mkdtemp(), name + '_temp_corpora_train_x.dat') #Update per experiment\n",
    "    file_corpora_test_x = path.join(mkdtemp(), name + '_temp_corpora_test_x.dat')\n",
    "    \n",
    "    #Shaping\n",
    "    shape_train_x = (len(corpora_train),max_len_sentences,embeddigs_cols,1)\n",
    "    shape_test_x = (len(corpora_test),max_len_sentences,embeddigs_cols,1)\n",
    "    \n",
    "    #Data sets\n",
    "    corpora_train_x = np.memmap(\n",
    "        filename = file_corpora_train_x,\n",
    "        dtype='float32',\n",
    "        mode='w+',\n",
    "        shape = shape_train_x)\n",
    "    \n",
    "    corpora_test_x = np.memmap( #Test Corpora (for future evaluation)\n",
    "        filename = file_corpora_test_x,\n",
    "        dtype='float32',\n",
    "        mode='w+',\n",
    "        shape = shape_test_x)\n",
    "    \n",
    "    target_train_y = np.array(target_train) #Train Target\n",
    "    target_test_y = np.array(target_test) #Test Target (for future evaluation)\n",
    "    \n",
    "    #Reshaping Train Inputs\n",
    "    for doc in range(len(corpora_train)):\n",
    "        for words_rows in range(corpora_train[doc].shape[0]):\n",
    "            embed_flatten = np.array(corpora_train[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "            for embedding_cols in range(embed_flatten.shape[0]):\n",
    "                corpora_train_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]\n",
    "                \n",
    "    #Reshaping Test Inputs (for future evaluation)\n",
    "    for doc in range(len(corpora_test)):\n",
    "        for words_rows in range(corpora_test[doc].shape[0]):\n",
    "            embed_flatten = np.array(corpora_test[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "            for embedding_cols in range(embed_flatten.shape[0]):\n",
    "                corpora_test_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]\n",
    "                \n",
    "    #Saving Test Data\n",
    "    if(save_file):\n",
    "        np.save(name + '/corpora_test_x.npy',corpora_test_x)\n",
    "        np.save(name + '/target_test_y.npy',target_test_y)\n",
    "    \n",
    "    return corpora_train_x, corpora_test_x, target_train_y, target_test_y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. Sentence # words: 527\n",
      "Mix. Sentence # words: 6\n"
     ]
    }
   ],
   "source": [
    "train, test, trY,trT = process_corpora(\"../data/small_dataset/\",name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from securereqnet.utils import Embeddings\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Input: List of strings to be vectorized\n",
    "    Output: List of vectorized strings in same order as input\"\"\"\n",
    "\n",
    "    embeddings = Embeddings()\n",
    "    embed_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),'data',\"word_embeddings-embed_size_100-epochs_100.csv\")\n",
    "    #embed_path = '../data/word_embeddings-embed_size_100-epochs_100.csv'\n",
    "    embeddings_dict = embeddings.get_embeddings_dict(embed_path)\n",
    "    inp_shape = (len(sentences), 618, 100, 1)\n",
    "    inp = np.zeros(shape=inp_shape, dtype='float32')\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        vectorized = embeddings.vectorize(sentence, embeddings_dict)\n",
    "     \n",
    "        for words_rows in range(vectorized.shape[0]):\n",
    "            embed_flatten = np.array(vectorized[words_rows]).flatten()\n",
    "            for embedding_cols in range(embed_flatten.shape[0]):\n",
    "                inp[i,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]\n",
    "        # print(inp)\n",
    "    return inp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
