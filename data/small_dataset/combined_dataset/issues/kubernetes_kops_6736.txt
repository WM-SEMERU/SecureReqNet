kops 1.11.1: after etcd-manager opt-in the last master "not yet joined cluster"


1. What kops version are you running? The command kops version, will display
this information.
Version 1.11.1 (git-0f2aa8d30)

2. What Kubernetes version are you running? kubectl version will print the
version if a cluster is running or provide the Kubernetes version specified as
a kops flag.
Client Version: version.Info{Major:"1", Minor:"12", GitVersion:"v1.12.7", GitCommit:"6f482974b76db3f1e0f5d24605a9d1d38fad9a2b", GitTreeState:"clean", BuildDate:"2019-03-25T02:52:13Z", GoVersion:"go1.10.8", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.8", GitCommit:"4e209c9383fa00631d124c8adcc011d617339b3c", GitTreeState:"clean", BuildDate:"2019-02-28T18:40:05Z", GoVersion:"go1.10.8", Compiler:"gc", Platform:"linux/amd64"}

kubectl get node
NAME                                        STATUS    ROLES     AGE       VERSION
ip-10-3-42-124.eu-west-1.compute.internal   Ready     master    1h        v1.11.8
ip-10-3-52-134.eu-west-1.compute.internal   Ready     node      2h        v1.11.8
ip-10-3-96-200.eu-west-1.compute.internal   Ready     master    1h        v1.11.8
10.3.85.190 - the 3rd master missing!

3. What cloud provider are you using?
aws
4. What commands did you run?  What is the simplest way to reproduce this issue?
Initial v1.10.6 cluster created with kops-1.10.0
kops create cluster \
       --authorization=RBAC \
       --api-loadbalancer-type=public \
       --bastion=false \
       --cloud=aws \
       --dns=public \
       --kubernetes-version="$K8S_VERSION" \
       --master-count=3 \
       --master-size=t2.small \
       --master-volume-size=64 \
       --master-zones=$KOPS_ZONES \
       --network-cidr="10.3.0.0/16" \
       --networking=weave  \
       --node-count=1 \
       --node-size=t2.small \
       --node-volume-size=64 \
       --ssh-access="$WHATSMYIP" \
       --ssh-public-key="kubernetes.$KOPS_CLUSTER_NAME.pub" \
       --topology=private \
       --vpc="vpc-xxx" \
       --zones="$KOPS_ZONES" \
       --name="$KOPS_CLUSTER_NAME" \
       --state="$KOPS_STATE_STORE" \
       --yes

Performed an update to k8s v1.11.8 using kops-1.11.1:
kops upgrade cluster --yes
kops update cluster --yes
kops rolling-update cluster --instance-group master-eu-west-1a --yes
kops rolling-update cluster --instance-group master-eu-west-1b --yes
kops rolling-update cluster --instance-group master-eu-west-1c --yes
kops rolling-update cluster --instance-group nodes --yes

Verified the etcd is in a healthy state.
Note: have not tried to reboot the nodes after the update - maybe the etcd failure would appear already at that stage.
Enabled etcd-manager
export KOPS_FEATURE_FLAGS=SpecOverrideFlag
kops set cluster cluster.spec.etcdClusters[*].provider=Manager
kops set cluster cluster.spec.etcdClusters[*].version=2.2.1
kops update cluster --yes
kops rolling-update cluster --yes

5. What happened after the commands executed?
The last master does not join the cluster. Some of the last info from etcd was
kubectl --namespace=kube-system exec etcd-manager-main-ip-10-3-42-124.eu-west-1.compute.internal -- /opt/etcd-v2.2.1-linux-amd64/etcdctl member list
client: etcd cluster is unavailable or misconfigured
command terminated with exit code 1

After several minutes the access to the api-server elb is lost (this could be an artifact of some unfortunate action while debugging, but the masters in the aws elb are "OutOfService").
6. What did you expect to happen?
I wanted to keep the k8s version at v1.11.8 and perform etcd-manager with etcd-2.2.1 opt-in as described at https://github.com/kubernetes/kops/blob/master/docs/releases/1.11-NOTES.md
and https://github.com/kubernetes/kops/blob/master/docs/etcd/manager.md
7. Please provide your cluster manifest. Execute
kops get --name my.example.com -o yaml to display your cluster manifest.
You may want to remove your cluster name and other sensitive information.
apiVersion: kops/v1alpha2
kind: Cluster
metadata:
  creationTimestamp: 2019-04-03T07:00:05Z
  name: etcd-manager.k8s.local
spec:
  api:
    loadBalancer:
      type: Public
  authorization:
    rbac: {}
  channel: stable
  cloudProvider: aws
  configBase: s3://clusters.etcd-manager.k8s.local/etcd-manager.k8s.local
  etcdClusters:
  - etcdMembers:
    - instanceGroup: master-eu-west-1a
      name: a
    - instanceGroup: master-eu-west-1b
      name: b
    - instanceGroup: master-eu-west-1c
      name: c
    name: main
    provider: Manager
    version: 2.2.1
  - etcdMembers:
    - instanceGroup: master-eu-west-1a
      name: a
    - instanceGroup: master-eu-west-1b
      name: b
    - instanceGroup: master-eu-west-1c
      name: c
    name: events
    provider: Manager
    version: 2.2.1
  iam:
    allowContainerRegistry: true
    legacy: false
  kubernetesApiAccess:
  - 0.0.0.0/0
  kubernetesVersion: 1.11.8
  masterPublicName: api.etcd-manager.k8s.local
  networkCIDR: 10.3.0.0/16
  networkID: vpc-xxx
  networking:
    weave:
      mtu: 8912
  nonMasqueradeCIDR: 100.64.0.0/10
  sshAccess:
  - 0.0.0.0/0
  subnets:
  - cidr: 10.3.32.0/19
    name: eu-west-1a
    type: Private
    zone: eu-west-1a
  - cidr: 10.3.64.0/19
    name: eu-west-1b
    type: Private
    zone: eu-west-1b
  - cidr: 10.3.96.0/19
    name: eu-west-1c
    type: Private
    zone: eu-west-1c
  - cidr: 10.3.0.0/22
    name: utility-eu-west-1a
    type: Utility
    zone: eu-west-1a
  - cidr: 10.3.4.0/22
    name: utility-eu-west-1b
    type: Utility
    zone: eu-west-1b
  - cidr: 10.3.8.0/22
    name: utility-eu-west-1c
    type: Utility
    zone: eu-west-1c
  topology:
    dns:
      type: Public
    masters: private
    nodes: private

---

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2019-04-03T07:00:05Z
  labels:
    kops.k8s.io/cluster: etcd-manager.k8s.local
  name: master-eu-west-1a
spec:
  image: kope.io/k8s-1.11-debian-stretch-amd64-hvm-ebs-2018-08-17
  machineType: t2.small
  maxSize: 1
  minSize: 1
  nodeLabels:
    kops.k8s.io/instancegroup: master-eu-west-1a
  role: Master
  rootVolumeSize: 64
  subnets:
  - eu-west-1a

---

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2019-04-03T07:00:05Z
  labels:
    kops.k8s.io/cluster: etcd-manager.k8s.local
  name: master-eu-west-1b
spec:
  image: kope.io/k8s-1.11-debian-stretch-amd64-hvm-ebs-2018-08-17
  machineType: t2.small
  maxSize: 1
  minSize: 1
  nodeLabels:
    kops.k8s.io/instancegroup: master-eu-west-1b
  role: Master
  rootVolumeSize: 64
  subnets:
  - eu-west-1b

---

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2019-04-03T07:00:05Z
  labels:
    kops.k8s.io/cluster: etcd-manager.k8s.local
  name: master-eu-west-1c
spec:
  image: kope.io/k8s-1.11-debian-stretch-amd64-hvm-ebs-2018-08-17
  machineType: t2.small
  maxSize: 1
  minSize: 1
  nodeLabels:
    kops.k8s.io/instancegroup: master-eu-west-1c
  role: Master
  rootVolumeSize: 64
  subnets:
  - eu-west-1c

---

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2019-04-03T07:00:05Z
  labels:
    kops.k8s.io/cluster: etcd-manager.k8s.local
  name: nodes
spec:
  image: kope.io/k8s-1.11-debian-stretch-amd64-hvm-ebs-2018-08-17
  machineType: t2.small
  maxSize: 1
  minSize: 1
  nodeLabels:
    kops.k8s.io/instancegroup: nodes
  role: Node
  rootVolumeSize: 64
  subnets:
  - eu-west-1a
  - eu-west-1b
  - eu-west-1c

8. Please run the commands with most verbose logging by adding the -v 10 flag.
Paste the logs into this report, or in a gist and provide the gist link here.
cat /var/log/etcd.log
...
I0405 13:24:42.946433    2762 peers.go:240] got ping response from etcd-events-a: info:<id:"etcd-events-a" endpoints:"10.3.42.124:3997" >
I0405 13:24:43.443049    2762 server.go:115] got ping info:<id:"etcd-events-b" endpoints:"10.3.85.190:3997" >
I0405 13:24:43.443195    2762 peers.go:240] got ping response from etcd-events-b: info:<id:"etcd-events-b" endpoints:"10.3.85.190:3997" >
I0405 13:24:43.460755    2762 peers.go:240] got ping response from etcd-events-c: info:<id:"etcd-events-c" endpoints:"10.3.96.200:3997" >
W0405 13:24:45.883773    2762 controller.go:714] health-check unable to reach member 56082fcc4aaa9005 on [http://etcd-events-b.internal.etcd-manager.k8s.local:4002]: context deadline exceeded
...

9. Anything else do we need to know?
On the newly started master nodes I see incorrect entries in /etc/hosts as mentioned in #6376:

the old entries for etcd-X.internal.etcd-manager.k8s.local etcd-events-X.internal.etcd-manager.k8s.local are present
the api.internal.etcd-manager.k8s.local entry for the 3rd master failed to join is missing

Contrary to the above issue, the cluster does not "fix" itself.
There is no network connectivity between the masters using the names from /etc/hosts due to the incorrect name entries in this file, but there is network connectivity between the 3 masters using directly the IP addresses. Interestingly the correct IP (new) addresses appear in /var/log/etcd.log (see above).
After manually fixing /etc/hosts, restarting the 3rd master that failed to join, and then the two remaining masters, one-by-one, the /etc/hosts restores itself with the incorrect contents, and there last master still does not join the cluster.
