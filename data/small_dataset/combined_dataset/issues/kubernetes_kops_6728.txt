Kops execContainer hook to install nvidia-docker creates orphaned containers


1. What kops version are you running? The command kops version, will display
this information.
1.11.1
2. What Kubernetes version are you running? kubectl version will print the
version if a cluster is running or provide the Kubernetes version specified as
a kops flag.
1.11.8
3. What cloud provider are you using?
AWS
4. What commands did you run?  What is the simplest way to reproduce this issue?

Create a GPU instance group (e.g. p2.xlarge) by following this https://github.com/kubernetes/kops/tree/master/hooks/nvidia-device-plugin#preferred-deviceplugin-gpu-mode.
Deploy Cluster Autoscaler AND Prometheus node exporter.
Deploy a pod that requests for nvidia.com/gpu: 1

5. What happened after the commands executed?

Cluster Autoscaler triggers scale up
GPU node is provisioned and proceeds to install nvidia drivers, nvidia docker runtime, etc. via kops execContainer hook.
Kubelet starts up and mark node as Ready.
At this point, there might be pods (containers) already running. For e.g. Prometheus node exporter.
Kops hook completes execution and restarts the kubelet service. Part of this step changes docker runtime to nvidia-docker runtime.
Suspect that containers are created by kubelet again. Prometheus node exporter cannot start as there's already another exporter that bound to port 9100:

î‚° kubectl logs -f prometheus-prometheus-node-exporter-fb65g -n monitoring --all-containers
time="2019-04-04T01:19:55Z" level=info msg="Starting node_exporter (version=0.16.0, branch=HEAD, revision=d42bd70f4363dced6b77d8fc311ea57b63387e4f)" source="node_exporter.go:82"
time="2019-04-04T01:19:55Z" level=info msg="Build context (go=go1.9.6, user=root@a67a9bc13a69, date=20180515-15:52:42)" source="node_exporter.go:83"
time="2019-04-04T01:19:55Z" level=info msg="Enabled collectors:" source="node_exporter.go:90"
time="2019-04-04T01:19:55Z" level=info msg=" - arp" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - bcache" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - bonding" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - conntrack" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - cpu" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - diskstats" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - edac" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - entropy" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - filefd" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - filesystem" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - hwmon" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - infiniband" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - ipvs" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - loadavg" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - mdadm" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - meminfo" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - netdev" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - netstat" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - nfs" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - nfsd" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - sockstat" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - stat" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - textfile" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - time" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - timex" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - uname" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - vmstat" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - wifi" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - xfs" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg=" - zfs" source="node_exporter.go:97"
time="2019-04-04T01:19:55Z" level=info msg="Listening on 0.0.0.0:9100" source="node_exporter.go:111"
time="2019-04-04T01:19:55Z" level=fatal msg="listen tcp 0.0.0.0:9100: bind: address already in use" source="node_exporter.go:114"


6. What did you expect to happen?
All pods run as expected.
7. Please provide your cluster manifest. Execute
kops get --name my.example.com -o yaml to display your cluster manifest.
You may want to remove your cluster name and other sensitive information.
apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2019-03-22T11:10:58Z
  labels:
    kops.k8s.io/cluster: xxxx.xxxx.xxxx.com
  name: gpu
spec:
  hooks:
  - execContainer:
      image: dcwangmit01/nvidia-device-plugin:0.1.0
  image: kope.io/k8s-1.10-debian-stretch-amd64-hvm-ebs-2018-05-27
  machineType: p2.xlarge
  maxPrice: "1.718"
  maxSize: 10
  minSize: 0
  nodeLabels:
    cluster-autoscaler.kubernetes.io/scaling-group: gpu
    kops.k8s.io/instancegroup: gpu
  role: Node
  subnets:
  - ap-southeast-1a
  - ap-southeast-1b
  suspendProcesses:
  - AZRebalance
  taints:
  - nvidia.com/gpu=:NoSchedule

8. Please run the commands with most verbose logging by adding the -v 10 flag.
Paste the logs into this report, or in a gist and provide the gist link here.
N/A
9. Anything else do we need to know?
Prometheus node exporter pod runs and binds to 9100 port on the host.
