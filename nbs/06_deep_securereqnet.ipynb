{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#danaderp May6'19\n",
    "#Prediction For Main Issues Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/danaderp/.conda/envs/drmccr_conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited tf.compat.v2.summary API due to missing TensorBoard installation\n",
      "Limited tf.summary API due to missing TensorBoard installation\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dot, Input, Dense, Reshape, LSTM, Conv2D, Flatten, MaxPooling1D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.layers import Embedding, Multiply, Subtract, Reshape\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model structure\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.read_data import Dynamic_Dataset,Processing_Dataset\n",
    "from vectorize_sentence import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/augmented_dataset/\"\n",
    "process_unit = Processing_Dataset(path)\n",
    "ground_truth = process_unit.get_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dynamic_Dataset(ground_truth, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, train = process_unit.get_test_and_training(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11612\n",
      "104510\n",
      "('(1,0)', 'OSRAM SYLVANIA Osram Lightify Home before 2016-07-26 allows remote attackers to execute arbitrary commands via TCP port 4000.')\n",
      "('(1,0)', 'The currently used Rails version, in the stable branch, is insecure\\n\\nYou should update the Gemfile.lock to hotfix this.\\n\\nhttp://weblog.rubyonrails.org/2014/2/18/Rails_3_2_17_4_0_3_and_4_1_0_beta2_have_been_released/')\n"
     ]
    }
   ],
   "source": [
    "print(len(test))\n",
    "print(len(train))\n",
    "print(test[0])\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test split verification\n",
    "#for elem in test:\n",
    "#    print(elem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesing Corpora\n",
    "embeddings = Embeddings()\n",
    "max_words = 5000 #<------- [Parameter]\n",
    "pre_corpora_train = [doc for doc in train if len(doc[1])< max_words]\n",
    "pre_corpora_test = [doc for doc in test if len(doc[1])< max_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103862\n",
      "11553\n"
     ]
    }
   ],
   "source": [
    "print(len(pre_corpora_train))\n",
    "print(len(pre_corpora_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path = 'datasets/word_embeddings-embed_size_100-epochs_100.csv'\n",
    "embeddings_dict = embeddings.get_embeddings_dict(embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_train = [embeddings.vectorize(doc[1], embeddings_dict) for doc in pre_corpora_train]#vectorization Inputs\n",
    "corpora_test = [embeddings.vectorize(doc[1], embeddings_dict) for doc in pre_corpora_test]#vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = [[int(list(doc[0])[1]),int(list(doc[0])[3])] for doc in pre_corpora_train]#vectorization Output\n",
    "target_test = [[int(list(doc[0])[1]),int(list(doc[0])[3])]for doc in pre_corpora_test]#vectorization Output\n",
    "#target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sentences_train = max([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "max_len_sentences_test = max([len(doc) for doc in corpora_test]) #<------- [Parameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. Sentence # words: 618\n"
     ]
    }
   ],
   "source": [
    "max_len_sentences = max(max_len_sentences_train,max_len_sentences_test)\n",
    "print(\"Max. Sentence # words:\",max_len_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = np.size(corpora_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BaseLine Architecture <-------\n",
    "embeddigs_cols = embed_size\n",
    "input_sh = (max_len_sentences,embeddigs_cols,1)\n",
    "#Selecting filters? \n",
    "#https://stackoverflow.com/questions/48243360/how-to-determine-the-filter-parameter-in-the-keras-conv2d-function\n",
    "#https://stats.stackexchange.com/questions/196646/what-is-the-significance-of-the-number-of-convolution-filters-in-a-convolutional\n",
    "\n",
    "N_filters = 512 # <-------- [HyperParameter] Powers of 2 Numer of Features\n",
    "K = 2 # <-------- [HyperParameter] Number of Classess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(618, 100, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline_model = Sequential()\n",
    "gram_input = Input(shape = input_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Convolutional Layer (1-gram)\n",
    "conv_filter_1_gram = Conv2D(filters= N_filters, input_shape=input_sh, activation='relu', \n",
    "                       kernel_size=(1,embeddigs_cols), padding='valid',data_format=\"channels_last\")(gram_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2sd Convolutional Layer (3-gram)\n",
    "conv_filter_3_gram = Conv2D(filters= N_filters, input_shape=input_sh, activation='relu', \n",
    "                       kernel_size=(3,embeddigs_cols), padding='valid')(gram_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Convolutional Layer (5-gram)\n",
    "conv_filter_5_gram = Conv2D(filters= N_filters, input_shape=input_sh, activation='relu', \n",
    "                       kernel_size=(5,embeddigs_cols), padding='valid')(gram_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 618, 1, 512)\n",
      "(None, 616, 1, 512)\n",
      "(None, 614, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "print(conv_filter_1_gram.shape)\n",
    "print(conv_filter_3_gram.shape)\n",
    "print(conv_filter_5_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "max_pool_1_gram = MaxPooling2D(pool_size=((max_len_sentences-1+1), 1), strides=None, padding='valid')(conv_filter_1_gram)\n",
    "max_pool_3_gram = MaxPooling2D(pool_size=((max_len_sentences-3+1), 1), strides=None, padding='valid')(conv_filter_3_gram)\n",
    "max_pool_5_gram = MaxPooling2D(pool_size=((max_len_sentences-5+1), 1), strides=None, padding='valid')(conv_filter_5_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 1, 512)\n",
      "(None, 1, 1, 512)\n",
      "(None, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "print(max_pool_1_gram.shape)\n",
    "print(max_pool_3_gram.shape)\n",
    "print(max_pool_5_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected layer\n",
    "fully_connected_1_gram = Flatten()(max_pool_1_gram)\n",
    "fully_connected_3_gram = Flatten()(max_pool_3_gram)\n",
    "fully_connected_5_gram = Flatten()(max_pool_5_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512)\n",
      "(None, 512)\n",
      "(None, 512)\n"
     ]
    }
   ],
   "source": [
    "print(fully_connected_1_gram.shape)\n",
    "print(fully_connected_3_gram.shape)\n",
    "print(fully_connected_5_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_connected_1_gram = Reshape((N_filters, 1, 1))(fully_connected_1_gram)\n",
    "fully_connected_3_gram = Reshape((N_filters, 1, 1))(fully_connected_3_gram)\n",
    "fully_connected_5_gram = Reshape((N_filters, 1, 1))(fully_connected_5_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 1, 1)\n",
      "(None, 512, 1, 1)\n",
      "(None, 512, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(fully_connected_1_gram.shape)\n",
    "print(fully_connected_3_gram.shape)\n",
    "print(fully_connected_5_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Phase of the Convolution\n",
    "# 1st Convolutional Layer (1-gram)\n",
    "conv_filter_1_gram = Conv2D(filters= 128, activation='relu', \n",
    "                       kernel_size=(1,1), padding='valid')(fully_connected_1_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 512, 1, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_filter_1_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2sd Convolutional Layer (3-gram)\n",
    "conv_filter_3_gram = Conv2D(filters= 128, activation='relu', \n",
    "                       kernel_size=(3,1), padding='valid')(fully_connected_3_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 510, 1, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_filter_3_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd Convolutional Layer (5-gram)\n",
    "conv_filter_5_gram = Conv2D(filters= 128, activation='relu', \n",
    "                       kernel_size=(5,1), padding='valid')(fully_connected_5_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 508, 1, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_filter_5_gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling Layer\n",
    "max_pool_1_gram = MaxPooling2D(pool_size=((N_filters-1+1), 1), strides=None, padding='valid')(conv_filter_1_gram)\n",
    "max_pool_3_gram = MaxPooling2D(pool_size=((N_filters-3+1), 1), strides=None, padding='valid')(conv_filter_3_gram)\n",
    "max_pool_5_gram = MaxPooling2D(pool_size=((N_filters-5+1), 1), strides=None, padding='valid')(conv_filter_5_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 1, 128)\n",
      "(None, 1, 1, 128)\n",
      "(None, 1, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "print(max_pool_1_gram.shape)\n",
    "print(max_pool_3_gram.shape)\n",
    "print(max_pool_5_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected layer\n",
    "fully_connected_1_gram = Flatten()(max_pool_1_gram)\n",
    "fully_connected_3_gram = Flatten()(max_pool_3_gram)\n",
    "fully_connected_5_gram = Flatten()(max_pool_5_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128)\n",
      "(None, 128)\n",
      "(None, 128)\n"
     ]
    }
   ],
   "source": [
    "print(fully_connected_1_gram.shape)\n",
    "print(fully_connected_3_gram.shape)\n",
    "print(fully_connected_5_gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_vector = layers.concatenate([fully_connected_1_gram, fully_connected_3_gram, \n",
    "                                    fully_connected_5_gram], axis=-1)\n",
    "\n",
    "integration_layer = Dropout(0.2)(merged_vector) # <-------- [HyperParameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Hidden Layer\n",
    "deep_dense_layer = Dense(N_filters, activation='relu')(integration_layer)\n",
    "deep_dense_layer = Dropout(0.2)(deep_dense_layer) # <-------- [HyperParameter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Dense(K, activation='softmax')(deep_dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criticality Model\n",
    "criticality_network = Model(inputs=[gram_input],outputs=[predictions]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 618, 100, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 618, 1, 512)  51712       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 616, 1, 512)  154112      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 614, 1, 512)  256512      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 512)          0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 512)    0           flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 512)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 512)          0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 512)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 512)    0           flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 512)    0           flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 512, 1)    0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 512, 1)    0           reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 512, 1)    0           reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 512, 1, 1)    0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 512, 1, 1)    0           reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 512, 1, 1)    0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 512, 1, 128)  256         reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 510, 1, 128)  512         reshape_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 508, 1, 128)  768         reshape_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 128)          0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 128)          0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 128)          0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 384)          0           flatten_12[0][0]                 \n",
      "                                                                 flatten_13[0][0]                 \n",
      "                                                                 flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          197120      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1026        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 662,018\n",
      "Trainable params: 662,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(criticality_network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seting up the Model\n",
    "criticality_network.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data set organization\n",
    "from tempfile import mkdtemp\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memoization \n",
    "file_corpora_train_x = path.join(mkdtemp(), 'f-res_temp_corpora_train_x.dat') #Update per experiment\n",
    "file_corpora_test_x = path.join(mkdtemp(), 'f-res_temp_corpora_test_x.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shaping\n",
    "shape_train_x = (len(corpora_train),max_len_sentences,embeddigs_cols,1)\n",
    "shape_test_x = (len(corpora_test),max_len_sentences,embeddigs_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data sets\n",
    "corpora_train_x = np.memmap(\n",
    "        filename = file_corpora_train_x, \n",
    "        dtype='float32', \n",
    "        mode='w+', \n",
    "        shape = shape_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_test_x = np.memmap( #Test Corpora (for future evaluation)\n",
    "        filename = file_corpora_test_x, \n",
    "        dtype='float32', \n",
    "        mode='w+', \n",
    "        shape = shape_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_y = np.array(target_train) #Train Target\n",
    "target_test_y = np.array(target_test) #Test Target (for future evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103862, 618, 100, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103862, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11553, 618, 100, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11553, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping Train Inputs\n",
    "for doc in range(len(corpora_train)):\n",
    "    #print(corpora_train[doc].shape[1])\n",
    "    for words_rows in range(corpora_train[doc].shape[0]):\n",
    "        embed_flatten = np.array(corpora_train[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "        for embedding_cols in range(embed_flatten.shape[0]):\n",
    "            corpora_train_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping Test Inputs (for future evaluation)\n",
    "for doc in range(len(corpora_test)):\n",
    "    for words_rows in range(corpora_test[doc].shape[0]):\n",
    "        embed_flatten = np.array(corpora_test[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "        for embedding_cols in range(embed_flatten.shape[0]):\n",
    "            corpora_test_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CheckPoints\n",
    "#csv_logger = CSVLogger(system+'_training.log')\n",
    "filepath = \"f-res/best_model.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "mc = ModelCheckpoint(filepath, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks_list = [es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 83089 samples, validate on 20773 samples\n",
      "Epoch 1/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9472\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.99649, saving model to f-res/best_model.hdf5\n",
      "83089/83089 [==============================] - 330s 4ms/sample - loss: 0.1510 - accuracy: 0.9472 - val_loss: 0.0075 - val_accuracy: 0.9965\n",
      "Epoch 2/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.1126 - accuracy: 0.9617\n",
      "Epoch 00002: val_accuracy did not improve from 0.99649\n",
      "83089/83089 [==============================] - 338s 4ms/sample - loss: 0.1126 - accuracy: 0.9617 - val_loss: 0.0109 - val_accuracy: 0.9955\n",
      "Epoch 3/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9652\n",
      "Epoch 00003: val_accuracy improved from 0.99649 to 0.99653, saving model to f-res/best_model.hdf5\n",
      "83089/83089 [==============================] - 346s 4ms/sample - loss: 0.1008 - accuracy: 0.9652 - val_loss: 0.0075 - val_accuracy: 0.9965\n",
      "Epoch 4/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9688\n",
      "Epoch 00004: val_accuracy improved from 0.99653 to 0.99832, saving model to f-res/best_model.hdf5\n",
      "83089/83089 [==============================] - 339s 4ms/sample - loss: 0.0906 - accuracy: 0.9688 - val_loss: 0.0048 - val_accuracy: 0.9983\n",
      "Epoch 5/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9717\n",
      "Epoch 00005: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 339s 4ms/sample - loss: 0.0816 - accuracy: 0.9717 - val_loss: 0.0103 - val_accuracy: 0.9957\n",
      "Epoch 6/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9740\n",
      "Epoch 00006: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 338s 4ms/sample - loss: 0.0738 - accuracy: 0.9740 - val_loss: 0.0062 - val_accuracy: 0.9978\n",
      "Epoch 7/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9768\n",
      "Epoch 00007: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 343s 4ms/sample - loss: 0.0659 - accuracy: 0.9768 - val_loss: 0.0080 - val_accuracy: 0.9968\n",
      "Epoch 8/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0572 - accuracy: 0.9799\n",
      "Epoch 00008: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 328s 4ms/sample - loss: 0.0572 - accuracy: 0.9799 - val_loss: 0.0067 - val_accuracy: 0.9976\n",
      "Epoch 9/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0548 - accuracy: 0.9813\n",
      "Epoch 00009: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0548 - accuracy: 0.9813 - val_loss: 0.0059 - val_accuracy: 0.9976\n",
      "Epoch 10/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9828\n",
      "Epoch 00010: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0481 - accuracy: 0.9828 - val_loss: 0.0080 - val_accuracy: 0.9973\n",
      "Epoch 11/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9841\n",
      "Epoch 00011: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 308s 4ms/sample - loss: 0.0460 - accuracy: 0.9841 - val_loss: 0.0107 - val_accuracy: 0.9965\n",
      "Epoch 12/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9860\n",
      "Epoch 00012: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0402 - accuracy: 0.9860 - val_loss: 0.0088 - val_accuracy: 0.9967\n",
      "Epoch 13/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9880\n",
      "Epoch 00013: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0334 - accuracy: 0.9880 - val_loss: 0.0115 - val_accuracy: 0.9966\n",
      "Epoch 14/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9882\n",
      "Epoch 00014: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0342 - accuracy: 0.9882 - val_loss: 0.0090 - val_accuracy: 0.9965\n",
      "Epoch 15/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9888\n",
      "Epoch 00015: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 308s 4ms/sample - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.0101 - val_accuracy: 0.9978\n",
      "Epoch 16/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0288 - accuracy: 0.9898\n",
      "Epoch 00016: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 315s 4ms/sample - loss: 0.0288 - accuracy: 0.9898 - val_loss: 0.0119 - val_accuracy: 0.9968\n",
      "Epoch 17/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9904\n",
      "Epoch 00017: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 369s 4ms/sample - loss: 0.0281 - accuracy: 0.9904 - val_loss: 0.0118 - val_accuracy: 0.9970\n",
      "Epoch 18/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0235 - accuracy: 0.9916\n",
      "Epoch 00018: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 370s 4ms/sample - loss: 0.0235 - accuracy: 0.9916 - val_loss: 0.0130 - val_accuracy: 0.9972\n",
      "Epoch 19/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0239 - accuracy: 0.9920\n",
      "Epoch 00019: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 372s 4ms/sample - loss: 0.0239 - accuracy: 0.9920 - val_loss: 0.0131 - val_accuracy: 0.9962\n",
      "Epoch 20/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9921\n",
      "Epoch 00020: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 381s 5ms/sample - loss: 0.0237 - accuracy: 0.9921 - val_loss: 0.0138 - val_accuracy: 0.9975\n",
      "Epoch 21/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9927\n",
      "Epoch 00021: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 373s 4ms/sample - loss: 0.0220 - accuracy: 0.9927 - val_loss: 0.0101 - val_accuracy: 0.9977\n",
      "Epoch 22/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9930\n",
      "Epoch 00022: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 373s 4ms/sample - loss: 0.0214 - accuracy: 0.9930 - val_loss: 0.0160 - val_accuracy: 0.9957\n",
      "Epoch 23/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9929\n",
      "Epoch 00023: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 382s 5ms/sample - loss: 0.0214 - accuracy: 0.9929 - val_loss: 0.0114 - val_accuracy: 0.9972\n",
      "Epoch 24/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9936\n",
      "Epoch 00024: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 375s 5ms/sample - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.0156 - val_accuracy: 0.9967\n",
      "Epoch 25/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9939\n",
      "Epoch 00025: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 373s 4ms/sample - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0186 - val_accuracy: 0.9958\n",
      "Epoch 26/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9940\n",
      "Epoch 00026: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 372s 4ms/sample - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.0124 - val_accuracy: 0.9969\n",
      "Epoch 27/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9939\n",
      "Epoch 00027: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 384s 5ms/sample - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.0118 - val_accuracy: 0.9963\n",
      "Epoch 28/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9949\n",
      "Epoch 00028: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 377s 5ms/sample - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0086 - val_accuracy: 0.9976\n",
      "Epoch 29/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9944\n",
      "Epoch 00029: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 374s 5ms/sample - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.0226 - val_accuracy: 0.9947\n",
      "Epoch 30/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 00030: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 376s 5ms/sample - loss: 0.0186 - accuracy: 0.9942 - val_loss: 0.0185 - val_accuracy: 0.9963\n",
      "Epoch 31/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9948\n",
      "Epoch 00031: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 382s 5ms/sample - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.0162 - val_accuracy: 0.9963\n",
      "Epoch 32/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9950\n",
      "Epoch 00032: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 375s 5ms/sample - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0152 - val_accuracy: 0.9971\n",
      "Epoch 33/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9953\n",
      "Epoch 00033: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 377s 5ms/sample - loss: 0.0154 - accuracy: 0.9953 - val_loss: 0.0151 - val_accuracy: 0.9974\n",
      "Epoch 34/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9954\n",
      "Epoch 00034: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 384s 5ms/sample - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0173 - val_accuracy: 0.9967\n",
      "Epoch 35/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9955\n",
      "Epoch 00035: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 375s 5ms/sample - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.0184 - val_accuracy: 0.9964\n",
      "Epoch 36/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9954\n",
      "Epoch 00036: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 375s 5ms/sample - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.0183 - val_accuracy: 0.9966\n",
      "Epoch 37/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9957\n",
      "Epoch 00037: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 376s 5ms/sample - loss: 0.0148 - accuracy: 0.9957 - val_loss: 0.0158 - val_accuracy: 0.9965\n",
      "Epoch 38/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 00038: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 384s 5ms/sample - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.0130 - val_accuracy: 0.9966\n",
      "Epoch 39/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0145 - accuracy: 0.9957\n",
      "Epoch 00039: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 376s 5ms/sample - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0206 - val_accuracy: 0.9960\n",
      "Epoch 40/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9963\n",
      "Epoch 00040: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 377s 5ms/sample - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.0157 - val_accuracy: 0.9969\n",
      "Epoch 41/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9957\n",
      "Epoch 00041: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 378s 5ms/sample - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0113 - val_accuracy: 0.9980\n",
      "Epoch 42/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9958\n",
      "Epoch 00042: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 382s 5ms/sample - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0203 - val_accuracy: 0.9960\n",
      "Epoch 43/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9957\n",
      "Epoch 00043: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 375s 5ms/sample - loss: 0.0148 - accuracy: 0.9957 - val_loss: 0.0172 - val_accuracy: 0.9968\n",
      "Epoch 44/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9962\n",
      "Epoch 00044: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 377s 5ms/sample - loss: 0.0137 - accuracy: 0.9962 - val_loss: 0.0267 - val_accuracy: 0.9948\n",
      "Epoch 45/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9960\n",
      "Epoch 00045: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 381s 5ms/sample - loss: 0.0136 - accuracy: 0.9960 - val_loss: 0.0157 - val_accuracy: 0.9971\n",
      "Epoch 46/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9962\n",
      "Epoch 00046: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 379s 5ms/sample - loss: 0.0135 - accuracy: 0.9962 - val_loss: 0.0190 - val_accuracy: 0.9957\n",
      "Epoch 47/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9959\n",
      "Epoch 00047: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 375s 5ms/sample - loss: 0.0140 - accuracy: 0.9959 - val_loss: 0.0145 - val_accuracy: 0.9963\n",
      "Epoch 48/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9964\n",
      "Epoch 00048: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 378s 5ms/sample - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.0243 - val_accuracy: 0.9950\n",
      "Epoch 49/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9967\n",
      "Epoch 00049: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 383s 5ms/sample - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.0203 - val_accuracy: 0.9963\n",
      "Epoch 50/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9962\n",
      "Epoch 00050: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 376s 5ms/sample - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.0140 - val_accuracy: 0.9974\n",
      "Epoch 51/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9960\n",
      "Epoch 00051: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 377s 5ms/sample - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.0114 - val_accuracy: 0.9968\n",
      "Epoch 52/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9964\n",
      "Epoch 00052: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 376s 5ms/sample - loss: 0.0128 - accuracy: 0.9964 - val_loss: 0.0157 - val_accuracy: 0.9965\n",
      "Epoch 53/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9965\n",
      "Epoch 00053: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 385s 5ms/sample - loss: 0.0128 - accuracy: 0.9965 - val_loss: 0.0164 - val_accuracy: 0.9965\n",
      "Epoch 54/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9972\n",
      "Epoch 00054: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 378s 5ms/sample - loss: 0.0105 - accuracy: 0.9972 - val_loss: 0.0213 - val_accuracy: 0.9964\n",
      "Epoch 55/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9967\n",
      "Epoch 00055: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 376s 5ms/sample - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.0254 - val_accuracy: 0.9961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9967\n",
      "Epoch 00056: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 382s 5ms/sample - loss: 0.0120 - accuracy: 0.9967 - val_loss: 0.0238 - val_accuracy: 0.9969\n",
      "Epoch 57/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9967\n",
      "Epoch 00057: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 384s 5ms/sample - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.0182 - val_accuracy: 0.9967\n",
      "Epoch 58/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9968\n",
      "Epoch 00058: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 382s 5ms/sample - loss: 0.0122 - accuracy: 0.9968 - val_loss: 0.0152 - val_accuracy: 0.9965\n",
      "Epoch 59/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9970\n",
      "Epoch 00059: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 373s 4ms/sample - loss: 0.0113 - accuracy: 0.9970 - val_loss: 0.0218 - val_accuracy: 0.9961\n",
      "Epoch 60/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9970\n",
      "Epoch 00060: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0119 - accuracy: 0.9970 - val_loss: 0.0273 - val_accuracy: 0.9955\n",
      "Epoch 61/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9966\n",
      "Epoch 00061: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.0272 - val_accuracy: 0.9954\n",
      "Epoch 62/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9970\n",
      "Epoch 00062: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.0219 - val_accuracy: 0.9959\n",
      "Epoch 63/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9970\n",
      "Epoch 00063: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0107 - accuracy: 0.9970 - val_loss: 0.0221 - val_accuracy: 0.9958\n",
      "Epoch 64/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 00064: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0117 - accuracy: 0.9970 - val_loss: 0.0300 - val_accuracy: 0.9942\n",
      "Epoch 65/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9968\n",
      "Epoch 00065: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0124 - accuracy: 0.9968 - val_loss: 0.0244 - val_accuracy: 0.9961\n",
      "Epoch 66/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 00066: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0260 - val_accuracy: 0.9950\n",
      "Epoch 67/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 00067: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0260 - val_accuracy: 0.9952\n",
      "Epoch 68/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9971\n",
      "Epoch 00068: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0123 - accuracy: 0.9971 - val_loss: 0.0231 - val_accuracy: 0.9963\n",
      "Epoch 69/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 00069: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 312s 4ms/sample - loss: 0.0109 - accuracy: 0.9970 - val_loss: 0.0287 - val_accuracy: 0.9935\n",
      "Epoch 70/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9967\n",
      "Epoch 00070: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0122 - accuracy: 0.9968 - val_loss: 0.0188 - val_accuracy: 0.9957\n",
      "Epoch 71/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9976\n",
      "Epoch 00071: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.0182 - val_accuracy: 0.9960\n",
      "Epoch 72/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9971\n",
      "Epoch 00072: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 304s 4ms/sample - loss: 0.0121 - accuracy: 0.9971 - val_loss: 0.0226 - val_accuracy: 0.9950\n",
      "Epoch 73/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 00073: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 312s 4ms/sample - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.0213 - val_accuracy: 0.9951\n",
      "Epoch 74/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9970\n",
      "Epoch 00074: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.0200 - val_accuracy: 0.9956\n",
      "Epoch 75/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9974\n",
      "Epoch 00075: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0111 - accuracy: 0.9974 - val_loss: 0.0270 - val_accuracy: 0.9936\n",
      "Epoch 76/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9974\n",
      "Epoch 00076: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0295 - val_accuracy: 0.9921\n",
      "Epoch 77/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9981\n",
      "Epoch 00077: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.0279 - val_accuracy: 0.9944\n",
      "Epoch 78/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9969\n",
      "Epoch 00078: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0318 - val_accuracy: 0.9923\n",
      "Epoch 79/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9972\n",
      "Epoch 00079: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.0283 - val_accuracy: 0.9938\n",
      "Epoch 80/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9974\n",
      "Epoch 00080: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0102 - accuracy: 0.9974 - val_loss: 0.0202 - val_accuracy: 0.9954\n",
      "Epoch 81/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9971\n",
      "Epoch 00081: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.0284 - val_accuracy: 0.9933\n",
      "Epoch 82/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9975\n",
      "Epoch 00082: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0105 - accuracy: 0.9975 - val_loss: 0.0346 - val_accuracy: 0.9923\n",
      "Epoch 83/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9973\n",
      "Epoch 00083: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.0180 - val_accuracy: 0.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9975\n",
      "Epoch 00084: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0106 - accuracy: 0.9975 - val_loss: 0.0227 - val_accuracy: 0.9960\n",
      "Epoch 85/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9978\n",
      "Epoch 00085: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0108 - accuracy: 0.9978 - val_loss: 0.0395 - val_accuracy: 0.9939\n",
      "Epoch 86/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9975\n",
      "Epoch 00086: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0108 - accuracy: 0.9975 - val_loss: 0.0206 - val_accuracy: 0.9966\n",
      "Epoch 87/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9974\n",
      "Epoch 00087: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0103 - accuracy: 0.9974 - val_loss: 0.0286 - val_accuracy: 0.9936\n",
      "Epoch 88/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9976\n",
      "Epoch 00088: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.0271 - val_accuracy: 0.9946\n",
      "Epoch 89/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9973\n",
      "Epoch 00089: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 311s 4ms/sample - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.0188 - val_accuracy: 0.9965\n",
      "Epoch 90/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9977\n",
      "Epoch 00090: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.0256 - val_accuracy: 0.9951\n",
      "Epoch 91/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9977\n",
      "Epoch 00091: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0101 - accuracy: 0.9977 - val_loss: 0.0234 - val_accuracy: 0.9954\n",
      "Epoch 92/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9974\n",
      "Epoch 00092: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0106 - accuracy: 0.9974 - val_loss: 0.0219 - val_accuracy: 0.9961\n",
      "Epoch 93/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9973\n",
      "Epoch 00093: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 305s 4ms/sample - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.0227 - val_accuracy: 0.9958\n",
      "Epoch 94/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9974\n",
      "Epoch 00094: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.0301 - val_accuracy: 0.9935\n",
      "Epoch 95/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9977\n",
      "Epoch 00095: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0094 - accuracy: 0.9977 - val_loss: 0.0247 - val_accuracy: 0.9948\n",
      "Epoch 96/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 00096: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.0151 - val_accuracy: 0.9968\n",
      "Epoch 97/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 00097: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 308s 4ms/sample - loss: 0.0101 - accuracy: 0.9976 - val_loss: 0.0242 - val_accuracy: 0.9954\n",
      "Epoch 98/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9976\n",
      "Epoch 00098: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 313s 4ms/sample - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.0254 - val_accuracy: 0.9958\n",
      "Epoch 99/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9973\n",
      "Epoch 00099: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0109 - accuracy: 0.9973 - val_loss: 0.0222 - val_accuracy: 0.9953\n",
      "Epoch 100/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9975\n",
      "Epoch 00100: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.0228 - val_accuracy: 0.9948\n",
      "Epoch 101/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9977\n",
      "Epoch 00101: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0087 - accuracy: 0.9977 - val_loss: 0.0287 - val_accuracy: 0.9940\n",
      "Epoch 102/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9973\n",
      "Epoch 00102: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 314s 4ms/sample - loss: 0.0107 - accuracy: 0.9973 - val_loss: 0.0257 - val_accuracy: 0.9958\n",
      "Epoch 103/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9978\n",
      "Epoch 00103: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 307s 4ms/sample - loss: 0.0087 - accuracy: 0.9978 - val_loss: 0.0216 - val_accuracy: 0.9958\n",
      "Epoch 104/2000\n",
      "83072/83089 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9974\n",
      "Epoch 00104: val_accuracy did not improve from 0.99832\n",
      "83089/83089 [==============================] - 306s 4ms/sample - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.0205 - val_accuracy: 0.9954\n",
      "Epoch 00104: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Model Fitting\n",
    "history = criticality_network.fit(\n",
    "            x = corpora_train_x, \n",
    "            y = target_train_y,\n",
    "            #batch_size=64,\n",
    "            epochs=2000, #5 <------ Hyperparameter\n",
    "            validation_split = 0.2,\n",
    "            callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Training History\n",
    "df_history = pd.DataFrame.from_dict(history.history)\n",
    "df_history.to_csv('f-res/history_training.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.947249</td>\n",
       "      <td>0.150975</td>\n",
       "      <td>0.996486</td>\n",
       "      <td>0.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.961680</td>\n",
       "      <td>0.112626</td>\n",
       "      <td>0.995475</td>\n",
       "      <td>0.010928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.965158</td>\n",
       "      <td>0.100765</td>\n",
       "      <td>0.996534</td>\n",
       "      <td>0.007520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968841</td>\n",
       "      <td>0.090637</td>\n",
       "      <td>0.998315</td>\n",
       "      <td>0.004767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971741</td>\n",
       "      <td>0.081611</td>\n",
       "      <td>0.995667</td>\n",
       "      <td>0.010286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      loss  val_accuracy  val_loss\n",
       "0  0.947249  0.150975      0.996486  0.007508\n",
       "1  0.961680  0.112626      0.995475  0.010928\n",
       "2  0.965158  0.100765      0.996534  0.007520\n",
       "3  0.968841  0.090637      0.998315  0.004767\n",
       "4  0.971741  0.081611      0.995667  0.010286"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_history.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Test Data\n",
    "np.save('f-res/corpora_test_x.npy',corpora_test_x)\n",
    "np.save('f-res/target_test_y.npy',target_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f07ec148940>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nziXk/wP2VzFPxSaKkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f092d223c50>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluation\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs2 = range(len(acc))\n",
    " \n",
    "plt.plot(epochs2, acc, 'b', label='Training')\n",
    "plt.plot(epochs2, val_acc, 'r', label='Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs2, loss, 'b', label='Training')\n",
    "plt.plot(epochs2, val_loss, 'r', label='Validation')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score,precision_recall_curve\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'f-res/best_model.hdf5'\n",
    "criticality_network_load = load_model(path) #<----- The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11553/11553 [==============================] - 11s 942us/sample - loss: 0.0897 - accuracy: 0.9691\n"
     ]
    }
   ],
   "source": [
    "score = criticality_network_load.evaluate(corpora_test_x, target_test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0896831521055043\n",
      "Test accuracy: 0.9690989\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 7.9988111e-10],\n",
       "       [7.7354908e-03, 9.9226451e-01],\n",
       "       [1.0000000e+00, 3.0692110e-10],\n",
       "       ...,\n",
       "       [4.3467302e-03, 9.9565327e-01],\n",
       "       [1.0000000e+00, 1.5595104e-08],\n",
       "       [9.1771345e-04, 9.9908233e-01]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_predict = criticality_network_load.predict(x=corpora_test_x)\n",
    "history_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_data = pd.DataFrame(history_predict,columns=list('AB'))\n",
    "target_data = pd.DataFrame(target_test_y,columns=list('LN'))\n",
    "data = target_data.join(inferred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(data['L'])\n",
    "y_score= list(data['A'])\n",
    "average_precision = average_precision_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.99\n"
     ]
    }
   ],
   "source": [
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.993\n"
     ]
    }
   ],
   "source": [
    "#ROC Curve (all our samples are balanced)\n",
    "auc = roc_auc_score(y_true, y_score)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
