Private topology forces me to create public subnets


**1. What kops version are you running?
Version 1.11.1 (git-0f2aa8d30)
**2. What Kubernetes version are you running?
Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-25T15:53:57Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
3. What cloud provider are you using?
aws
**4. What commands did you run?
I am launching this script for the generation of my cluster
export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)
export NODE_SIZE='m4.large'
export MASTER_SIZE='m4.large'
export NODE_NUMBER=4
export REGION='eu-west-1'
export ZONES='eu-west-1a,eu-west-1b,eu-west-1c'
export AWS_S3_BUCKET_NAME='kops-s3-bucket-....'
export CLUSTER_NAME='poc.aws.cluster.k8s.local'
export KUBERNETES_NETWORKING='calico'
export VPC_AWS_ID='vpc-077....'
export TOPOLOGY='private'
export LOAD_BALANCER_TYPE='internal'
export ASSOCIATE_PUBLIC_IP=false
export DNS='private'
aws s3api create-bucket 
--bucket "$AWS_S3_BUCKET_NAME" 
--region "$REGION" 
--create-bucket-configuration LocationConstraint="$REGION"
aws s3api put-bucket-versioning --bucket "$AWS_S3_BUCKET_NAME" --versioning-configuration Status=Enabled
kops create cluster 
--node-count $NODE_NUMBER 
--zones $ZONES 
--node-size $NODE_SIZE 
--master-size $MASTER_SIZE 
--master-zones $ZONES 
--networking $KUBERNETES_NETWORKING 
--topology $TOPOLOGY 
--api-loadbalancer-type $LOAD_BALANCER_TYPE 
--associate-public-ip=$ASSOCIATE_PUBLIC_IP 
--dns $DNS 
--vpc $VPC_AWS_ID 
--name $CLUSTER_NAME \
5. What happened after the commands executed?
The cluster is generated incorrectly for my use case. I have specified that the cluster is private, with internal load balancer and private dns. ALL PRIVATE. After launching the command generates 3 private subnets (correct), but generates 3 other public subnets / utility, 3 elastic IP's and the public subnets connected to an Internet gateway accessible via the Internet.
6. What did you expect to happen?
I try to create a private cluster, closed to the internet, only accessible from my corporate vpn connection to the vpc, but I can not find a configuration in kops that allows me to do this.
**7. Please provide your cluster manifest. Execute
apiVersion: kops/v1alpha2
kind: Cluster
metadata:
creationTimestamp: 2019-04-02T12:12:50Z
name: poc.aws.cluster.k8s.local
spec:
api:
loadBalancer:
type: Internal
authorization:
rbac: {}
channel: stable
cloudProvider: aws
configBase: s3://kops-s3-bucket-....
etcdClusters:

etcdMembers:

instanceGroup: master-eu-west-1a
name: a
instanceGroup: master-eu-west-1b
name: b
instanceGroup: master-eu-west-1c
name: c
name: main
version: 3.2.24


etcdMembers:

instanceGroup: master-eu-west-1a
name: a
instanceGroup: master-eu-west-1b
name: b
instanceGroup: master-eu-west-1c
name: c
name: events
version: 3.2.24
iam:
allowContainerRegistry: true
legacy: false
kubelet:
anonymousAuth: false
kubernetesApiAccess:


0.0.0.0/0
kubernetesVersion: 1.11.8
masterPublicName: api.poc.aws.cluster.k8s.local
networkCIDR: 10.48.0.0/16
networkID: vpc-077102...
networking:
calico:
majorVersion: v3
nonMasqueradeCIDR: 100.64.0.0/10
sshAccess:
0.0.0.0/0
subnets:
cidr: 10.48.32.0/19
name: eu-west-1a
type: Private
zone: eu-west-1a
cidr: 10.48.64.0/19
name: eu-west-1b
type: Private
zone: eu-west-1b
cidr: 10.48.96.0/19
name: eu-west-1c
type: Private
zone: eu-west-1c
cidr: 10.48.0.0/22
name: utility-eu-west-1a
type: Utility
zone: eu-west-1a
cidr: 10.48.4.0/22
name: utility-eu-west-1b
type: Utility
zone: eu-west-1b
cidr: 10.48.8.0/22
name: utility-eu-west-1c
type: Utility
zone: eu-west-1c
topology:
dns:
type: Private
masters: private
nodes: private

8. Please run the commands with most verbose logging by adding the -v 10 flag.
Paste the logs into this report, or in a gist and provide the gist link here.
9. Anything else do we need to know?
