{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# default_exp preprocessing\n",
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "#nbdev_default_export preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#danaderp May6'19\n",
    "#Prediction For Main Issues Data Set\n",
    "import csv\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dot, Input, Dense, Reshape, LSTM, Conv2D, Flatten, MaxPooling1D, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.layers import Embedding, Multiply, Subtract\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# visualize model structure\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Embeddings' from 'securereqnet.utils' (/home/roger/Desktop/SoftwareEngineering/SecureReqNet/securereqnet/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d5c48a389bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msecurereqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDynamic_Dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mProcessing_Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msecurereqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Embeddings' from 'securereqnet.utils' (/home/roger/Desktop/SoftwareEngineering/SecureReqNet/securereqnet/utils.py)"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from securereqnet.utils import Dynamic_Dataset, Processing_Dataset\n",
    "from securereqnet.utils import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def __read_dataset(path):\n",
    "    process_unit = Processing_Dataset(path)\n",
    "    ground_truth = process_unit.get_ground_truth()\n",
    "    dataset = Dynamic_Dataset(ground_truth, path,False) # I'm not sure this needs to be False. RC\n",
    "    return process_unit, ground_truth, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__read_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a9a772f77c95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#../data replaces datasets for the to access data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../data/augmented_dataset/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprocess_unit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__read_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name '__read_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#../data replaces datasets for the to access data\n",
    "path = \"../data/augmented_dataset/\"\n",
    "process_unit, ground_truth, dataset = __read_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nbdev_export` not found.\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "def __train_test_split(process_unit, ground_truth,isZipFile):\n",
    "    test, train = process_unit.get_test_and_training(ground_truth,isZip = isZipFile)\n",
    "    return test,train\n",
    "#As the data is stored in a zip file isZip = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11612\n",
      "104510\n",
      "('(1,0)', b'Strip style attributes from HTML tags in add-on description\\n\\n\\nThe old addons-server code stripped all HTML tag attributes (with some exceptions). This would strip out any style attributes. Even though we\\'re using a dom-purify on addons-frontend, it doesn\\'t strip style attributes. I don\\'t know how serious it is because I can\\'t think of a real XSS. However, there may be a possibility by crafting an add-on that has something like this in the description:\\n<b style=\"background-image: url(javascript:alert(document.cookie))\">placeholder</b>')\n",
      "('(1,0)', b'The currently used Rails version, in the stable branch, is insecure\\n\\nYou should update the Gemfile.lock to hotfix this.\\n\\nhttp://weblog.rubyonrails.org/2014/2/18/Rails_3_2_17_4_0_3_and_4_1_0_beta2_have_been_released/')\n"
     ]
    }
   ],
   "source": [
    "print(len(test))\n",
    "print(len(train))\n",
    "print(test[0])\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test split verification\n",
    "#for elem in test:\n",
    "#    print(elem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Roger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "#Added due to a lookup error in the next cell\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-61137111d45a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nbdev_export'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Preprocesing Corpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m \u001b[1;31m#<------- [Parameter]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpre_corpora_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "#export\n",
    "def __create_corpora(test,train):\n",
    "    '''Creates the corpora for training. Returns corpora_train,corpora_test,max_len_sentences,embed_size'''\n",
    "    embeddings = Embeddings()\n",
    "    max_words = 5000 #<------- [Parameter]\n",
    "    pre_corpora_train = [doc for doc in train if len(doc[1])< max_words]\n",
    "    pre_corpora_test = [doc for doc in test if len(doc[1])< max_words]\n",
    "\n",
    "    embed_path = '../data/word_embeddings-embed_size_100-epochs_100.csv'\n",
    "    embeddings_dict = embeddings.get_embeddings_dict(embed_path)\n",
    "\n",
    "    # .decode(\"utf-8\") takes the doc's which are saved as byte files and converts them into strings for tokenization\n",
    "    corpora_train = [embeddings.vectorize(doc[1].decode(\"utf-8\"), embeddings_dict) for doc in pre_corpora_train]#vectorization Inputs\n",
    "    corpora_test = [embeddings.vectorize(doc[1].decode(\"utf-8\"), embeddings_dict) for doc in pre_corpora_test]#vectorization\n",
    "\n",
    "    target_train = [[int(list(doc[0])[1]),int(list(doc[0])[3])] for doc in pre_corpora_train]#vectorization Output\n",
    "    target_test = [[int(list(doc[0])[1]),int(list(doc[0])[3])]for doc in pre_corpora_test]#vectorization Output\n",
    "    #target_train\n",
    "\n",
    "    max_len_sentences_train = max([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "    max_len_sentences_test = max([len(doc) for doc in corpora_test]) #<------- [Parameter]\n",
    "\n",
    "    max_len_sentences = max(max_len_sentences_train,max_len_sentences_test)\n",
    "    print(\"Max. Sentence # words:\",max_len_sentences)\n",
    "\n",
    "    min_len_sentences_train = min([len(doc) for doc in corpora_train]) #<------- [Parameter]\n",
    "    min_len_sentences_test = min([len(doc) for doc in corpora_test]) #<------- [Parameter]\n",
    "\n",
    "    min_len_sentences = max(min_len_sentences_train,min_len_sentences_test)\n",
    "    print(\"Mix. Sentence # words:\",min_len_sentences)\n",
    "\n",
    "    embed_size = np.size(corpora_train[0][0])\n",
    "    \n",
    "    return corpora_train,corpora_test,max_len_sentences,embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_training_corpora(path,isZip):\n",
    "    process_unit, ground_truth = __read_dataset(path)\n",
    "    test, train = __train_test_split(process_unit, ground_truth,isZip)\n",
    "    return __create_corpora(test,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Data set organization\n",
    "from tempfile import mkdtemp\n",
    "import os.path as path\n",
    "def process_corpora(corpora_train,corpora_test,save_file=False,path=\"\",name=\"\"):\n",
    "    '''\n",
    "    Process the corpora data for model training. Takes in corpora_train,corpora_test,save,path,name.\n",
    "    \n",
    "    @param save_file (bool): Determine if the data should be saved on disk.\n",
    "    \n",
    "    @param path (string): Path to where the dataset should be save to.\n",
    "    \n",
    "    @param name (string): Name of the model used to name files saved.   \n",
    "    '''\n",
    "    #Memoization\n",
    "    file_corpora_train_x = path.join(mkdtemp(), name + '_temp_corpora_train_x.dat') #Update per experiment\n",
    "    file_corpora_test_x = path.join(mkdtemp(), name + '_temp_corpora_test_x.dat')\n",
    "    \n",
    "    #Shaping\n",
    "    shape_train_x = (len(corpora_train),max_len_sentences,embeddigs_cols,1)\n",
    "    shape_test_x = (len(corpora_test),max_len_sentences,embeddigs_cols,1)\n",
    "    \n",
    "    #Data sets\n",
    "    corpora_train_x = np.memmap(\n",
    "        filename = file_corpora_train_x,\n",
    "        dtype='float32',\n",
    "        mode='w+',\n",
    "        shape = shape_train_x)\n",
    "    \n",
    "    corpora_test_x = np.memmap( #Test Corpora (for future evaluation)\n",
    "        filename = file_corpora_test_x,\n",
    "        dtype='float32',\n",
    "        mode='w+',\n",
    "        shape = shape_test_x)\n",
    "    \n",
    "    target_train_y = np.array(target_train) #Train Target\n",
    "    target_test_y = np.array(target_test) #Test Target (for future evaluation)\n",
    "    \n",
    "    #Reshaping Train Inputs\n",
    "    for doc in range(len(corpora_train)):\n",
    "        for words_rows in range(corpora_train[doc].shape[0]):\n",
    "            embed_flatten = np.array(corpora_train[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "            for embedding_cols in range(embed_flatten.shape[0]):\n",
    "                corpora_train_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]\n",
    "                \n",
    "    #Reshaping Test Inputs (for future evaluation)\n",
    "    for doc in range(len(corpora_test)):\n",
    "        for words_rows in range(corpora_test[doc].shape[0]):\n",
    "            embed_flatten = np.array(corpora_test[doc][words_rows]).flatten() #<--- Capture doc and word\n",
    "            for embedding_cols in range(embed_flatten.shape[0]):\n",
    "                corpora_test_x[doc,words_rows,embedding_cols,0] = embed_flatten[embedding_cols]\n",
    "                \n",
    "    #Saving Test Data\n",
    "    if(save_file):\n",
    "        np.save(name + '/corpora_test_x.npy',corpora_test_x)\n",
    "        np.save(name + '/target_test_y.npy',target_test_y)\n",
    "    \n",
    "    return corpora_train_x, corpora_test_x, target_train_y, target_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
