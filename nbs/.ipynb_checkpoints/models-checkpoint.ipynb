{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_model(model):\n",
    "    '''\n",
    "    Returns a pretrained version of the selected model\n",
    "    \n",
    "    Users can select: alpha    \n",
    "    '''\n",
    "    path = '../pretrained_models/'+model'.hdf5'\n",
    "    criticality_network_load = load_model(path) #<----- The Model'\n",
    "    return criticality_network_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_alpha(embed_size,max_len_sentences):\n",
    "    '''Creates a securereqnet alpha model and compiles it'''\n",
    "    #BaseLine Architecture <-------\n",
    "    embeddigs_cols = embed_size\n",
    "    input_sh = (max_len_sentences,embeddigs_cols,1)\n",
    "    #Selecting filters? \n",
    "    #https://stackoverflow.com/questions/48243360/how-to-determine-the-filter-parameter-in-the-keras-conv2d-function\n",
    "    #https://stats.stackexchange.com/questions/196646/what-is-the-significance-of-the-number-of-convolution-filters-in-a-convolutional\n",
    "\n",
    "    N_filters = 128 # <-------- [HyperParameter] Powers of 2 Numer of Features\n",
    "    K = 2 # <-------- [HyperParameter] Number of Classess\n",
    "\n",
    "    #baseline_model = Sequential()\n",
    "    gram_input = Input(shape = input_sh)\n",
    "\n",
    "    # 1st Convolutional Layer Convolutional Layer (7-gram)\n",
    "    conv_1_layer = Conv2D(filters=32, input_shape=input_sh, activation='relu', \n",
    "                          kernel_size=(7,embeddigs_cols), padding='valid')(gram_input)\n",
    "\n",
    "    # Max Pooling \n",
    "    max_1_pooling = MaxPooling2D(pool_size=((max_len_sentences-7+1),1), strides=None, padding='valid')(conv_1_layer)\n",
    "\n",
    "    # Fully Connected layer\n",
    "    fully_connected_1_gram = Flatten()(max_1_pooling)\n",
    "\n",
    "    fully_connected_1_gram = Reshape((32, 1, 1))(fully_connected_1_gram)\n",
    "\n",
    "    # 2nd Convolutional Layer (5-gram)\n",
    "    conv_2_layer = Conv2D(filters=64, kernel_size=(5,1), activation='relu', \n",
    "                          padding='valid')(fully_connected_1_gram)\n",
    "\n",
    "    max_2_pooling = MaxPooling2D(pool_size=((32-5+1),1), strides=None, padding='valid')(conv_2_layer)\n",
    "\n",
    "    # Fully Connected layer\n",
    "    fully_connected_2_gram = Flatten()(max_2_pooling)\n",
    "\n",
    "    fully_connected_2_gram = Reshape((64, 1, 1))(fully_connected_2_gram)\n",
    "\n",
    "    # 3rd Convolutional Layer (3-gram)\n",
    "    conv_3_layer =  Conv2D(filters=128, kernel_size=(3,1), activation='relu', \n",
    "                          padding='valid')(fully_connected_2_gram)\n",
    "\n",
    "    # 4th Convolutional Layer (3-gram)\n",
    "    conv_4_layer = Conv2D(filters=128, kernel_size=(3,1), activation='relu', \n",
    "                         padding='valid')(conv_3_layer)\n",
    "\n",
    "    # 5th Convolutional Layer (3-gram)\n",
    "    conv_5_layer = Conv2D(filters=64, kernel_size=(3,1), activation='relu', \n",
    "                         padding='valid')(conv_4_layer)\n",
    "\n",
    "    # Max Pooling\n",
    "    max_5_pooling = MaxPooling2D(pool_size=(58,1), strides=None, padding='valid')(conv_5_layer)\n",
    "\n",
    "    # Fully Connected layer\n",
    "    fully_connected = Flatten()(max_5_pooling)\n",
    "\n",
    "    # 1st Fully Connected Layer\n",
    "    deep_dense_1_layer = Dense(32, activation='relu')(fully_connected)\n",
    "    deep_dense_1_layer = Dropout(0.2)(deep_dense_1_layer) # <-------- [HyperParameter]\n",
    "\n",
    "    # 2nd Fully Connected Layer\n",
    "    deep_dense_2_layer = Dense(32, activation='relu')(deep_dense_1_layer)\n",
    "    deep_dense_2_layer = Dropout(0.2)(deep_dense_2_layer) # <-------- [HyperParameter]\n",
    "\n",
    "    # 3rd Fully Connected Layer\n",
    "    deep_dense_3_layer = Dense(16, activation='relu')(deep_dense_2_layer)\n",
    "    deep_dense_3_layer = Dropout(0.2)(deep_dense_3_layer) # <-------- [HyperParameter]\n",
    "\n",
    "    predictions = Dense(K, activation='softmax')(deep_dense_3_layer)\n",
    "\n",
    "    #Criticality Model\n",
    "    criticality_network = Model(inputs=[gram_input],outputs=[predictions]) \n",
    "    print(criticality_network.summary())\n",
    "    \n",
    "    #Seting up the Model\n",
    "    criticality_network.compile(optimizer='adam',loss='binary_crossentropy',\n",
    "                                  metrics=['accuracy'])\n",
    "    \n",
    "    return criticality_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(corpora_train_x,train_y,criticality_network,save_model=True,save_history=True,path=''):\n",
    "    '''\n",
    "    Fits a securereqnet model to the data\n",
    "    \n",
    "    Returns model, history\n",
    "    \n",
    "    @param save_model (bool): Saves the model to the input path\n",
    "    \n",
    "    @param save_history (bool): Saves the history to the path\n",
    "    \n",
    "    @param path (string): Path to where the model and/or history will be saved to\n",
    "    '''\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    mc = ModelCheckpoint(filepath, monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    callbacks_list = [es,mc]\n",
    "\n",
    "    #Model Fitting\n",
    "    history = criticality_network.fit(\n",
    "            x = corpora_train_x, \n",
    "            y = target_train_y,\n",
    "            #batch_size=64,\n",
    "            epochs=2000, #5 <------ Hyperparameter\n",
    "            validation_split = 0.2,\n",
    "            callbacks=callbacks_list\n",
    "    )\n",
    "    \n",
    "    #CheckPoints\n",
    "    #csv_logger = CSVLogger(system+'_training.log')\n",
    "    # filepath changed from: \"alex-adapted-res-003/best_model.hdf5\" for testing\n",
    "    # The folder alex-adapted-res-003 doesn't exist yet in the repository. RC created 08_test in the root folder \n",
    "    # manually\n",
    "    if(save_model):\n",
    "        filepath = path+\"/best_model.hdf5\"\n",
    "        criticality_network.save(filepath)\n",
    "    \n",
    "    # filepath changed from: 'alex-adapted-res-003/history_training.csv' for testing\n",
    "    #Saving Training History\n",
    "    if(save_history):\n",
    "        df_history = pd.DataFrame.from_dict(history.history)\n",
    "        df_history.to_csv(path+'/history_training.csv', encoding='utf-8',index=False)\n",
    "        df_history.head()\n",
    "    \n",
    "    return criticality_network, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from securereqnet.preprocessing import *\n",
    "path = \"../data/augmented_dataset/\"\n",
    "corpora_train,corpora_test,max_len_sentences,embed_size = get_training_corpora(path,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_train_x, corpora_test_x, target_train_y, target_test_y = process_corpora(corpora_train,corpora_test,save_file=True,path=\"../alpha\",name=\"alpha-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"alpha\")\n",
    "model.evaluate(corpora_test_x, target_test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(corpora_train_x,train_y,criticality_network,save_model=True,save_history=True,path='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
