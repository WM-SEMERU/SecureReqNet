Kubelet with anonymous-auth documentation is laking




What kops version are you running?
Version 1.7.1


What Kubernetes version are you running?
kubectl version
Client Version: version.Info{Major:"1", Minor:"7", GitVersion:"v1.7.4", GitCommit:"793658f2d7ca7f064d2bdf606519f9fe1229c381", GitTreeState:"clean", BuildDate:"2017-08-17T08:48:23Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"darwin/amd64"}


What cloud provider are you using?
AWS


What commands did you run?  What is the simplest way to reproduce this issue?


kops create cluster --dns-zone=lab-aws.jossctz-test.com --zones=us-east-1a,us-east-1b --name=k8s-kops.lab-aws.jossctz-test.com
kops edit cluster k8s-kops.lab-aws.jossctz-test.com
# Then I add to the spec section:
kubelet:
  anonymousAuth: false
# And finally
kops update cluster k8s-kops.lab-aws.jossctz-test.com --yes



What happened after the commands executed?
The cluster isn't working. Kubectl fails connecting to the API Server: kubectl get pods returns an IO Timeout.


What did you expect to happen?
My cluster is running and safe from https://github.com/kayrus/kubelet-exploit kind of attacks.


Please provide your cluster manifest.


apiVersion: kops/v1alpha2
kind: Cluster
metadata:
  creationTimestamp: 2017-11-19T15:15:36Z
  name: k8s-kops.lab-aws.jossctz-test.com
spec:
  api:
    dns: {}
  authorization:
    alwaysAllow: {}
  channel: stable
  cloudProvider: aws
  configBase: s3://jossctz/k8s-kops.lab-aws.jossctz-test.com
  dnsZone: lab-aws.jossctz-test.com
  etcdClusters:
  - etcdMembers:
    - instanceGroup: master-us-east-1a
      name: a
    name: main
  - etcdMembers:
    - instanceGroup: master-us-east-1a
      name: a
    name: events
  kubelet:
    anonymousAuth: false
  kubernetesApiAccess:
  - 0.0.0.0/0
  kubernetesVersion: 1.7.10
  masterInternalName: api.internal.k8s-kops.lab-aws.jossctz-test.com
  masterPublicName: api.k8s-kops.lab-aws.jossctz-test.com
  networkCIDR: 172.20.0.0/16
  networking:
    kubenet: {}
  nonMasqueradeCIDR: 100.64.0.0/10
  sshAccess:
  - 0.0.0.0/0
  subnets:
  - cidr: 172.20.32.0/19
    name: us-east-1a
    type: Public
    zone: us-east-1a
  - cidr: 172.20.64.0/19
    name: us-east-1b
    type: Public
    zone: us-east-1b
  topology:
    dns:
      type: Public
    masters: public
    nodes: public

---

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2017-11-19T15:15:36Z
  labels:
    kops.k8s.io/cluster: k8s-kops.lab-aws.jossctz-test.com
  name: master-us-east-1a
spec:
  image: kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28
  machineType: m3.medium
  maxSize: 1
  minSize: 1
  role: Master
  subnets:
  - us-east-1a

---

apiVersion: kops/v1alpha2
kind: InstanceGroup
metadata:
  creationTimestamp: 2017-11-19T15:15:36Z
  labels:
    kops.k8s.io/cluster: k8s-kops.lab-aws.jossctz-test.com
  name: nodes
spec:
  image: kope.io/k8s-1.7-debian-jessie-amd64-hvm-ebs-2017-07-28
  machineType: t2.medium
  maxSize: 2
  minSize: 2
  role: Node
  subnets:
  - us-east-1a
  - us-east-1b


I want to setup a cluster with kops where pods can't talk directly to the kubelet. I think I have to set anonymousAuth: false and it should work but it doesn't.
I looked in other issues and the documentation, tried a few things but nothing worked.
Maybe the procedure should be easier to find/more explicit.
