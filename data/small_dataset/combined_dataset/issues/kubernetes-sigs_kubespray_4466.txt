err='error: unable to recognize \"/etc/kubernetes/k8s-cluster-critical-pc.yml\"


Is this a BUG REPORT or FEATURE REQUEST? (choose one):
BUG REPORT
Environment:

Cloud provider or hardware configuration:

3  baremetal hp servers
BIOS Information
Vendor: HP
Version: P70
Release Date: 08/02/2014
Address: 0xF0000
Runtime Size: 64 kB
ROM Size: 8192 kB

OS (printf "$(uname -srm)\n$(cat /etc/os-release)\n"):

Linux 4.4.0-131-generic x86_64
NAME="Ubuntu"
VERSION="16.04.5 LTS (Xenial Xerus)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 16.04.5 LTS"
VERSION_ID="16.04"
HOME_URL="http://www.ubuntu.com/"
SUPPORT_URL="http://help.ubuntu.com/"
BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/"
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial

Version of Ansible (ansible --version):

ansible 2.7.10
config file = /home/user1/kubespray/ansible.cfg
configured module search path = [u'/home/user1/kubespray/library']
ansible python module location = /usr/local/lib/python2.7/dist-packages/ansible
executable location = /usr/local/bin/ansible
python version = 2.7.12 (default, Nov 12 2018, 14:36:49) [GCC 5.4.0 20160609]
Kubespray version (commit) (git rev-parse --short HEAD):
d8a023a
Network plugin used:
Calico
Copy of your inventory file:
[all]
seba21 ansible_host=135.25.24.53  ip=135.25.24.53 ansible_user=root
seba22 ansible_host=135.25.24.47  ip=135.25.24.47 ansible_user=root
seba23 ansible_host=135.25.24.45  ip=135.25.24.45 ansible_user=root
## configure a bastion host if your nodes are not directly reachable
bastion ansible_host=x.x.x.x ansible_user=some_user
[kube-master]
seba21
seba22
seba23
[etcd]
seba21
seba22
seba23
[kube-node]
seba21
seba22
seba23
[k8s-cluster:children]
kube-master
kube-node
[calico-rr]
Command used to invoke ansible:
ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml
Output of ansible run:
TASK [kubernetes-apps/cluster_roles : PriorityClass | Create k8s-cluster-critical] *************************************************************************************
Monday 08 April 2019  11:34:43 -0400 (0:00:00.678)       0:07:21.543 **********
fatal: [seba23]: FAILED! => {"changed": false, "msg": "error running kubectl (/usr/local/bin/kubectl apply --force --filename=/etc/kubernetes/k8s-cluster-critical-pc.yml) command (rc=1), out='', err='error: unable to recognize "/etc/kubernetes/k8s-cluster-critical-pc.yml": Get https://135.25.24.45:6443/api?timeout=32s: Service Unavailable\n'"}
NO MORE HOSTS LEFT *****************************************************************************************************************************************************
to retry, use: --limit @/home/user1/kubespray/cluster.retry
PLAY RECAP *************************************************************************************************************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=0
seba21                     : ok=315  changed=24   unreachable=0    failed=0
seba22                     : ok=287  changed=23   unreachable=0    failed=0
seba23                     : ok=288  changed=23   unreachable=0    failed=1
Monday 08 April 2019  11:34:46 -0400 (0:00:03.019)       0:07:24.562 **********
download : container_download | download images for kubeadm config images -------------------------------------------------------------------------------------- 25.07s
etcd : Configure | reload systemd ------------------------------------------------------------------------------------------------------------------------------- 7.84s
download : container_download | Download containers if pull is required or told to always pull (all nodes) ------------------------------------------------------ 7.21s
container-engine/docker : ensure docker packages are installed -------------------------------------------------------------------------------------------------- 6.62s
etcd : Configure | reload systemd ------------------------------------------------------------------------------------------------------------------------------- 6.18s
kubernetes/master : kubeadm | Remove taint for master with node role -------------------------------------------------------------------------------------------- 6.05s
kubernetes/master : kubeadm | write out kubeadm certs ----------------------------------------------------------------------------------------------------------- 5.39s
kubernetes/preinstall : Get current version of calico cluster version ------------------------------------------------------------------------------------------- 5.37s
gather facts from all instances --------------------------------------------------------------------------------------------------------------------------------- 5.28s
kubernetes/preinstall : Create kubernetes directories ----------------------------------------------------------------------------------------------------------- 4.29s
kubernetes/master : Generate new certs and keys ----------------------------------------------------------------------------------------------------------------- 3.74s
kubernetes/node : Modprode Kernel Module for IPVS --------------------------------------------------------------------------------------------------------------- 3.43s
kubernetes/preinstall : Stop if known booleans are set as strings (Use JSON format on CLI: -e "{'key': true }") ------------------------------------------------- 3.34s
kubernetes-apps/cluster_roles : PriorityClass | Create k8s-cluster-critical ------------------------------------------------------------------------------------- 3.02s
kubernetes/preinstall : Create cni directories ------------------------------------------------------------------------------------------------------------------ 2.83s
kubernetes/master : Generate new configuration files ------------------------------------------------------------------------------------------------------------ 2.76s
kubernetes/node : Enable bridge-nf-call tables ------------------------------------------------------------------------------------------------------------------ 2.72s
download : container_download | Download containers if pull is required or told to always pull (all nodes) ------------------------------------------------------ 2.71s
download : container_download | Download containers if pull is required or told to always pull (all nodes) ------------------------------------------------------ 2.68s
download : container_download | Download containers if pull is required or told to always pull (all nodes) ------------------------------------------------------ 2.67s
Anything else do we need to know:
I ran the play book several times and got the same error, except one time I got an error:
TASK [etcd : Configure | Check if member is in etcd cluster] ***********************************************************************************************************
Monday 08 April 2019  11:26:25 -0400 (0:00:00.206)       0:05:06.803 **********
ok: [seba22]
ok: [seba23]
fatal: [seba21]: FAILED! => {"changed": false, "cmd": "/usr/local/bin/etcdctl --no-sync --endpoints=https://135.25.24.53:2379,https://135.25.24.47:2379,https://135.25.24.45:2379 member list | grep -q 135.25.24.53", "delta": "0:00:05.025110", "end": "2019-04-08 11:26:31.610039", "msg": "non-zero return code", "rc": 1, "start": "2019-04-08 11:26:26.584929", "stderr": "context deadline exceeded", "stderr_lines": ["context deadline exceeded"], "stdout": "", "stdout_lines": []}
...ignoring
TASK [etcd : Configure | Check if member is in etcd-events cluster] ****************************************************************************************************
Monday 08 April 2019  11:26:31 -0400 (0:00:05.922)       0:05:12.726 **********
TASK [etcd : Configure | Join member(s) to etcd cluster one at a time] *************************************************************************************************
Monday 08 April 2019  11:26:31 -0400 (0:00:00.205)       0:05:12.932 **********
included: /home/user1/kubespray/roles/etcd/tasks/join_etcd_member.yml for seba21 => (item=seba21)
TASK [etcd : Join Member | Add member to etcd cluster] *****************************************************************************************************************
Monday 08 April 2019  11:26:32 -0400 (0:00:00.333)       0:05:13.265 **********
FAILED - RETRYING: Join Member | Add member to etcd cluster (4 retries left).
FAILED - RETRYING: Join Member | Add member to etcd cluster (3 retries left).
FAILED - RETRYING: Join Member | Add member to etcd cluster (2 retries left).
FAILED - RETRYING: Join Member | Add member to etcd cluster (1 retries left).
fatal: [seba21]: FAILED! => {"attempts": 4, "changed": true, "cmd": "/usr/local/bin/etcdctl --endpoints=https://135.25.24.53:2379,https://135.25.24.47:2379,https://135.25.24.45:2379 member add etcd1 https://135.25.24.53:2380", "delta": "0:00:00.348231", "end": "2019-04-08 11:26:55.288667", "msg": "non-zero return code", "rc": 1, "start": "2019-04-08 11:26:54.940436", "stderr": "membership: peerURL exists", "stderr_lines": ["membership: peerURL exists"], "stdout": "", "stdout_lines": []}
