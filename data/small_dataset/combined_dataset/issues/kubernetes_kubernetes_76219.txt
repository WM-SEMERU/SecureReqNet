Server-side apply protobuf serialization performance issue


Context
Kubernetes 1.14, ServerSideApply feature enabled on kube-apiserver.
Introduction
Server-Side Apply (SSA) seems to be triggering a performance issue on protobuf serialization of objects, specifically while serializing the new managedFields recursive map. We detected the problem because the scalability tests mostly indicated problems in LISTing objects (pods and nodes), even though SSA shouldn't impact this code path.
At this point the problems are probably related to the size of the objects (increased significantly by the additional field) which would lead to:

more serialization/deserialization,
more data transfer between client and apiserver,
more data transfer between apiserver and etcd

Probably many other things, let's try to reproduce.
Steps to reproduce
I'm now comparing the following to set-up:
# Running without the feature enabled
STORAGE_MEDIA_TYPE=application/vnd.kubernetes.protobuf \ 
FEATURE_GATES=ServerSideApply=false \
PATH=$PWD/third_party/etcd:$PATH \
hack/local-up-cluster.sh -O

# Running with the feature enabled
STORAGE_MEDIA_TYPE=application/vnd.kubernetes.protobuf \
FEATURE_GATES=ServerSideApply=true \
PATH=$PWD/third_party/etcd:$PATH \
hack/local-up-cluster.sh -O
I first create a random deployment with a 1000 replicas, the pods don't have to actually start, but should be successfully created.
My benchmark is to run the following python3 script:
import urllib.request
import time
import statistics

request = urllib.request.Request(
    "http://localhost:8080/api/v1/pods",
    headers={
        "Accept": "application/vnd.kubernetes.protobuf",
    }
)

size = []
duration = []
NUM_RUNS = 50
for _ in range(NUM_RUNS):
  b = time.perf_counter()
  size += [len(urllib.request.urlopen(request).read())]
  duration += [(time.perf_counter() - b) * 1000]

print("%d runs:" % NUM_RUNS)
print("size min/max/avg/stddev: %d/%d/%d/%d" % (
    min(size), max(size), statistics.mean(size), statistics.stdev(size)
))
print("duration min/max/avg/stddev: %d/%d/%d/%d" % (
    min(duration),
    max(duration),
    statistics.mean(duration),
    statistics.stdev(duration)
))
Without the feature-flag, the results look like this (YMMV):
> python3 test.py 
50 runs:
size min/max/avg/stddev: 980436/980470/980450/17
duration min/max/avg/stddev: 22/47/31/5
with the feature-flag, the results looks like this:
50 runs:
size min/max/avg/stddev: 2465211/2465211/2465211/0
duration min/max/avg/stddev: 97/146/110/13
Investigation
Since we can see that there is a clear difference between the two, let's try to profile the difference.
Let's send queries infinitely to the apiserver so we can see what it's doing:
while true
do curl --silent -H "Accept: application/vnd.kubernetes.protobuf" \
  http://localhost:8080/api/v1/pods >/dev/null
done
In a different window, start profiling:
go tool pprof http://localhost:8080/debug/pprof/profile?seconds=30
... (wait)
> kcachegrind # If you don't know this, it's absolutely awesome.

Let's look at that method:
https://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go#L2107-L2120
It basically does a full tree traversal to know how big the buffer needs to be allocated, that's OK.
The problem is that this method is called again and again at every depth of the tree while serializing the map:
https://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go#L737-L774
which means that the marshaling complexity is going to be proportional to the square of the depth of the map.
My suspicion thought is that this problem is not limited to "maps", but to all objects we serialize in kubernetes, let's look at it:
Taking a random other types from the same generated files (but you can look at any other protobuf generated files, they always do the same thing):
https://github.com/kubernetes/kubernetes/blob/v1.14.0/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/generated.pb.go#L1672-L1711
We can see that it's computing the size of the entire sub-object (by recursing as deeply as it can) before serializing each individual field, which are all  going to recurse AGAIN the entire object, and again and again ...
Detecting these with profiling is much harder because they don't all call the same method (each field has a different type). Running on the recursive map made it obvious because all the calls are aggregated under the same function.
Solutions
Marshaling protobuf objects requires to marshal the size of sub-objects before the actual bytes of these objects. Also this size is marshaled as a "varint", which prevents you from computing the bytes and then update the size in place. You would have to copy the data very frequently in order to do that.
An alternate, good solution is to memoize the size of each object so that you don't have to re-compute them very frequently. Interestingly, https://github.com/golang/protobuf does add a XXX_CacheSize int variable in go objects when it generates the go code from proto files. Ideally, the memoization would be "local" to the individual serialization process to avoid problems of invalidating the cache, running multiple serialization at the same time, or modifying objects themselves.
Next step would probably be to come-up with a relevant experiment to see how bad this actually is, both on the SSA map and on the rest of the objects.
