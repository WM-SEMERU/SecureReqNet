{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "import sys\n",
    "import os.path\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\"@danaderp May'20 Refactoring for enhancing time complexity with pandas vectorization\"\n",
    "\n",
    "class Dynamic_Dataset:\n",
    "\t\"\"\"\n",
    "\tThis class efficiently 'stores' a dataset. Only a list of filenames and\n",
    "\tmappings to their ground truth values are stored in memory. The file\n",
    "\tcontents are only brought into memory when requested.\n",
    "\n",
    "\tThis class supports indexing, slicing, and iteration.\n",
    "\n",
    "\tA user can treat an instance of this class exactly as they would a list.\n",
    "\tIndexing an instance of this class will return a tuple consisting of\n",
    "\tthe ground truth value and the file content of the filename at that index.\n",
    "\n",
    "\tA user can request the filename at an index with get_id(index)\n",
    "\n",
    "\tExample:\n",
    "\n",
    "\t\tdataset = Dynamic_Dataset(ground_truth)\n",
    "\n",
    "\t\tprint(dataset.get_id(0))\n",
    "\t\t\t-> gitlab_79.txt\n",
    "\n",
    "\t\tprint(dataset[0])\n",
    "\t\t\t-> ('(1,0)', 'The currently used Rails version, in the stable ...\n",
    "\n",
    "\t\tfor x in dataset[2:4]:\n",
    "\t\t\tprint(x)\n",
    "\t\t\t\t-> ('(1,0)', \"'In my attempt to add 2 factor authentication ...\n",
    "\t\t\t\t-> ('(1,0)', 'We just had an admin accidentally push to a ...\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, ground_truth, path, isZip):\n",
    "\t\t'''\n",
    "\t\t@param ground_truth (dict): A dictionary mapping filenames to ground truth values\n",
    "\t\t'''\n",
    "\t\tself.__keys = list(ground_truth.keys())\n",
    "\t\tself.__ground_truth = ground_truth\n",
    "\t\tself.__path = path\n",
    "\t\tself.__isZip = isZip\n",
    "\n",
    "\tdef __get_issue(self, filename):\n",
    "\t\tif self.__isZip:\n",
    "\t\t\tpaths = [str(x) for x in Path(self.__path).glob(\"**/*.zip\")]\n",
    "\t\t\tfor onezipath in paths:\n",
    "\t\t\t\tarchive = zipfile.ZipFile( onezipath, 'r')\n",
    "\t\t\t\tcontents = archive.read('issues/' + filename)\n",
    "\t\telse:\n",
    "\t\t\twith open(self.__path+'issues/' + filename, 'r') as file:\n",
    "\t\t\t\tcontents = file.read()\n",
    "\t\treturn contents.strip()\n",
    "\n",
    "\tdef get_id(self, index):\n",
    "\t\treturn self.__keys[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.__keys)\n",
    "\n",
    "\tdef __setitem__(self, key, item):\n",
    "\t\traise ValueError\n",
    "\n",
    "\tdef __getitem__(self, key):\n",
    "\t\tif type(key) == slice:\n",
    "\t\t\tnew_keys = self.__keys[key.start:key.stop:key.step]\n",
    "\t\t\tnew_gt = dict()\n",
    "\t\t\tfor key in new_keys:\n",
    "\t\t\t\tnew_gt[key] = self.__ground_truth[key]\n",
    "\t\t\treturn Dynamic_Dataset(new_gt)\n",
    "\t\telse:\n",
    "\t\t\tid = self.__keys[key]\n",
    "\t\t\treturn (self.__ground_truth[id], self.__get_issue(id))\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\tself.__index = 0\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __next__(self):\n",
    "\t\tif self.__index < len(self.__keys):\n",
    "\t\t\tto_return = self[self.__index]\n",
    "\t\t\tself.__index += 1\n",
    "\t\t\treturn to_return\n",
    "\t\telse:\n",
    "\t\t\traise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Dynamic_Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Processing_Dataset:\n",
    "    \"\"\"\n",
    "    A class to wrap up processing functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.__path = path\n",
    "\n",
    "    def get_issue(self, filename):\n",
    "        with open('combined_dataset/issues/' + filename, 'r') as file:\n",
    "            contents = file.read()\n",
    "        return contents.strip()\n",
    "\n",
    "    def get_ground_truth(self):\n",
    "        gt = dict()\n",
    "        #print(sys.path[0])\n",
    "        #path = \"combined_dataset/full_ground_truth.txt\"\n",
    "        #path = os.path.join(sys.path[0], path)\n",
    "        with open(self.__path+'full_ground_truth.txt') as gt_file:\n",
    "            for line in gt_file.readlines():\n",
    "                tokens = line.split()\n",
    "                filename = tokens[0]\n",
    "                security_status = tokens[1]\n",
    "                if filename in gt:\n",
    "                    raise KeyError(\"Invalid Ground Truth: Duplicate issue [{}]\".format(filename))\n",
    "                gt[filename] = security_status\n",
    "        return gt\n",
    "\n",
    "    def get_test_and_training(self, ground_truth, test_ratio=0.1, isZip = False):\n",
    "        ids = list(ground_truth.keys())\n",
    "        sr = []\n",
    "        nsr = []\n",
    "\n",
    "        for id in ids:\n",
    "            if ground_truth[id] == '(1,0)':\n",
    "                sr.append(id)\n",
    "            elif ground_truth[id] == '(0,1)':\n",
    "                nsr.append(id)\n",
    "            else:\n",
    "                raise ValueError(\"There was an issue with ground truth: {} - {}\".format(id, ground_truth[id]))\n",
    "\n",
    "\n",
    "        n_test = int(len(sr) * test_ratio)\n",
    "        sr_test = random.sample(sr, n_test)\n",
    "        nsr_test = random.sample(nsr, n_test)\n",
    "\n",
    "        test_gt = dict()\n",
    "        train_gt = dict(ground_truth)\n",
    "\n",
    "        for i in range(n_test):\n",
    "            sr.remove(sr_test[i])\n",
    "            test_gt[sr_test[i]] = '(1,0)'\n",
    "            del train_gt[sr_test[i]]\n",
    "\n",
    "            nsr.remove(nsr_test[i])\n",
    "            test_gt[nsr_test[i]] = '(0,1)'\n",
    "            del train_gt[nsr_test[i]]\n",
    "\n",
    "        test = Dynamic_Dataset(test_gt,self.__path, isZip)\n",
    "        train = Dynamic_Dataset(train_gt,self.__path, isZip)\n",
    "\n",
    "        return (test, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing. RC leaving for testing crew to use as a reference\n",
    "#if __name__ == '__main__':\n",
    "#ground_truth = get_ground_truth()\n",
    "#dataset = Dynamic_Dataset(ground_truth)\n",
    "\n",
    "#test, train = get_test_and_training(ground_truth)\n",
    "\n",
    "#print(test[0])\n",
    "#print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_Embeddings.ipynb.\n",
      "Converted 02_ Statistical-test [bootstrapping].ipynb.\n",
      "Converted 03_Clustering.ipynb.\n",
      "Converted 04_alpha_securereqnet[colab].ipynb.\n",
      "Converted 05_A_shallow_securereqnet-train.ipynb.\n",
      "Converted 05_shallow_securereqnet.ipynb.\n",
      "Converted 06_deep_securereqnet.ipynb.\n",
      "Converted 07_alex_securereqnet.ipynb.\n",
      "Converted 08_alpha_securereqnet.ipynb.\n",
      "Converted 09_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
